{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project: Control in a Continuous Action Space with DDPG\n",
    "\n",
    "In this project, we will explore the Deep Deterministic Policy Gradient (DDPG) algorithm, which is designed to handle continuous action spaces in Reinforcement Learning. We will use the Pendulum-v1 environment implemented in OpenAI Gym to implement the DDPG algorithm from scratch to solve the classical control problem of stabilizing an inverted pendulum. Throughout the development, we will incrementally build the components of DDPG and analyze their importance for correct and effective learning. This Jupyter notebook contains our implementation and report for this project. \n",
    "\n",
    "The instructions are available in `Miniproject_DDPG.pdf` file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from helpers import NormalizedEnv\n",
    "from helpers import RandomAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environement instance, an instance of the Pendulum-v1 environment wrapped in a NormalizedEnv class\n",
    "# to normalize the action space between -1 and 1\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "env = NormalizedEnv(env)\n",
    "\n",
    "# Create a RandomAgent\n",
    "random_agent = RandomAgent(env)\n",
    "\n",
    "# Function that run the agent for a given number of episodes and return the average reward\n",
    "def run_agent(agent, env, num_episodes):\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            action = agent.compute_action(state)\n",
    "            state, reward, _, trunc, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if trunc:\n",
    "                break\n",
    "        rewards.append(episode_reward)\n",
    "    return np.mean(rewards)\n",
    "average_reward = run_agent(random_agent, env, 10)\n",
    "print('Average reward over 10 episodes: {}'.format(average_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeuristicPendulumAgent:\n",
    "    def __init__(self, env: NormalizedEnv, torque_intensity: float=1.0) -> None:\n",
    "        \"\"\"Heuristic agent for the Pendulum-v1 environment.\"\"\"\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.torque_intensity = torque_intensity\n",
    "        \n",
    "    def compute_action(self, state):\n",
    "        \"\"\"Compute the action to apply to the environment.\n",
    "\n",
    "        When the pendulum is in the lower half of the domain, applies a fixed torque in the same direction as the pendulum's angular velocity.\n",
    "        When the pendulum is in the upper half of the domain, applies a fixed torque in the opposite direction as the pendulum's angular velocity.\n",
    "        \"\"\"\n",
    "        action = np.where(state[..., 0] < 0, # If the pendulum is in the lower half of the circle\n",
    "                        np.sign(state[..., 2]) * self.torque_intensity, \n",
    "                        -np.sign(state[..., 2]) * self.torque_intensity)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best torque intensity\n",
    "torque_intensities = np.linspace(0.1, 1, 10)\n",
    "rewards = []\n",
    "for torque_intensity in torque_intensities:\n",
    "    heuristic_agent = HeuristicPendulumAgent(env, torque_intensity=torque_intensity)\n",
    "    rewards.append(run_agent(heuristic_agent, env, 10))\n",
    "\n",
    "plt.plot(torque_intensities, rewards)\n",
    "plt.xlabel('Torque intensity')\n",
    "plt.ylabel('Average reward over 10 episodes')\n",
    "plt.show()\n",
    "\n",
    "# Create a HeuristicPendulumAgent with the best torque intensity\n",
    "heuristic_agent = HeuristicPendulumAgent(env, torque_intensity=torque_intensities[np.argmax(rewards)])\n",
    "# Print best reward over 10 episodes and associated torque intensity\n",
    "print('Best torque intensity: {}'.format(torque_intensities[np.argmax(rewards)]))\n",
    "print('Best reward over 10 episodes: {}'.format(np.max(rewards)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation of the Heuristic Agent using PIL\n",
    "state, _ = env.reset()\n",
    "frames = []\n",
    "while True:\n",
    "    action = heuristic_agent.compute_action(state)\n",
    "    state, reward, _, trunc, _ = env.step(action)\n",
    "    frames.append(env.render())\n",
    "    if trunc:\n",
    "        break\n",
    "\n",
    "frames = np.array(frames)\n",
    "frames = [Image.fromarray(frame) for frame in frames]\n",
    "frames[0].save(\"heuristic_agent.gif\", save_all=True, append_images=frames[1:], duration=50, loop=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q function of the heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"Replay buffer used to store a sequence of transitions.\n",
    "\n",
    "        A transition is a tuple of the form (state, action, reward, next_state, trunc).\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def store(self, transition: tuple):\n",
    "        \"\"\"Store a transition in the buffer.\"\"\"\n",
    "        self.buffer.append(transition)\n",
    "        if len(self.buffer) > self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "\n",
    "    def sample(self, batch_size: int=128) -> list:\n",
    "        \"\"\"Sample a batch of transitions from the buffer.\"\"\"\n",
    "        batch = []\n",
    "        indexes = np.sort(np.random.choice(len(self.buffer), \n",
    "                                           size=min(batch_size, len(self.buffer)), \n",
    "                                           replace=False))[::-1]\n",
    "        for index in indexes:\n",
    "            batch.append(self.buffer[index])\n",
    "            self.buffer.pop(index)\n",
    "        return batch\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int=3, action_size: int=1):\n",
    "        \"\"\"QNetwork. Maps (state, action) pairs to Q-values.\"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): State tensor\n",
    "            action (torch.Tensor): Action tensor\n",
    "\n",
    "        Returns:\n",
    "            q_value (torch.Tensor): Q-value tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def TD_learning_update(q_network, agent, optimizer, transitions, gamma):\n",
    "    \"\"\"Perform a 1-step TD-learning update for a batch of transitions.\n",
    "\n",
    "    Args:\n",
    "        q_network (QNetwork): QNetwork instance\n",
    "        agent (Agent): Agent (i.e. Policy) instance\n",
    "        optimizer (torch.optim): Optimizer instance\n",
    "        transitions (List[Transition]): Batch of transitions\n",
    "        gamma (float): Discount factor\n",
    "\n",
    "    Returns:\n",
    "        loss (float): Loss value\n",
    "    \"\"\"\n",
    "    # Iterate over all the transitions and store each state, \n",
    "    # action, reward, next_state and trunc in a numpy array\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    truncs = []\n",
    "    for transition in transitions:\n",
    "        states.append(transition[0])\n",
    "        actions.append(transition[1])\n",
    "        rewards.append(transition[2])\n",
    "        next_states.append(transition[3])\n",
    "        truncs.append(transition[4])\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    next_states = np.array(next_states)\n",
    "    truncs = np.array(truncs)\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    # Note: We choose the convention that all the tensors are of shape \n",
    "    # (batch_size, *), where * is the shape of the data and of dtype\n",
    "    # torch.float32, except for truncs which is of dtype torch.bool\n",
    "    states_t = torch.from_numpy(states).float()\n",
    "    actions_t = torch.from_numpy(actions)[:, None].float()\n",
    "    rewards_t = torch.from_numpy(rewards)[:, None].float()\n",
    "    next_states_t = torch.from_numpy(next_states).float()\n",
    "    truncs_t = torch.from_numpy(truncs)[:, None].bool()\n",
    "\n",
    "    next_actions = agent.compute_action(next_states)\n",
    "    next_actions_t = torch.as_tensor(next_actions)[:, None].float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Naive next Q-values (i.e. without taking truncation into account)\n",
    "        naive_next_qs = q_network(next_states_t, next_actions_t)\n",
    "        # Actual next Q-values (i.e. taking truncation into account)\n",
    "        next_qs = torch.where(truncs_t, \n",
    "                              torch.zeros_like(rewards_t), \n",
    "                              naive_next_qs)\n",
    "        gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "        targets = rewards_t + gamma * next_qs\n",
    "\n",
    "    loss = F.mse_loss(q_network(states_t, actions_t), targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "q_network = QNetwork(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0])\n",
    "optimizer = torch.optim.SGD(q_network.parameters(), lr=1e-4)\n",
    "gamma = 0.99\n",
    "\n",
    "# Training loop for the Q-learning\n",
    "losses = []\n",
    "for episode in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    while True:\n",
    "        action = heuristic_agent.compute_action(state)\n",
    "        next_state, reward, _, trunc, _ = env.step(action)\n",
    "        replay_buffer.store((state, action, reward, next_state, trunc))\n",
    "        if trunc:\n",
    "            break\n",
    "        state = next_state\n",
    "    # Perform a TD-learning update every 5 episodes\n",
    "    if episode % 5 == 4:\n",
    "        transitions = replay_buffer.sample(128)\n",
    "        loss = TD_learning_update(q_network, heuristic_agent, optimizer, transitions, gamma)\n",
    "        losses.append(loss.item())\n",
    "        print('Episode: {}, Loss: {}'.format(episode, loss))\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_losses = losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
