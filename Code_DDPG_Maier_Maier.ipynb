{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project: Control in a Continuous Action Space with DDPG\n",
    "\n",
    "In this project, we will explore the Deep Deterministic Policy Gradient (DDPG) algorithm, which is designed to handle continuous action spaces in Reinforcement Learning. We will use the [Pendulum-v1](https://www.gymlibrary.dev/environments/classic_control/pendulum/) environment implemented in OpenAI Gym to implement the DDPG algorithm from scratch to solve the classical control problem of stabilizing an inverted pendulum. Throughout the development, we will incrementally build the components of DDPG and analyze their importance for correct and effective learning. \n",
    "\n",
    "This Jupyter notebook contains our implementation and report for this project. Do not forget to remove the report parts before submitting the notebook.\n",
    "\n",
    "The instructions are available in `Miniproject_DDPG.pdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import gym\n",
    "from helpers import NormalizedEnv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device: 'cpu' or 'cuda'\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create environement instance, an instance of the Pendulum-v1 environment wrapped in a NormalizedEnv class\n",
    "# to normalize the action space between -1 and 1\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "env = NormalizedEnv(env)\n",
    "\n",
    "# Number of steps per episode, see documentation of the Pendulum-v1 environment\n",
    "n_steps = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO docstring\n",
    "class Agent:\n",
    "    def __init__(self, env: NormalizedEnv) -> None:\n",
    "        \"\"\"Abstract agent class.\"\"\"\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "    def compute_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, env: NormalizedEnv) -> None:\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def compute_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        size = state.shape[:-1] + (self.action_size,)\n",
    "        return np.random.uniform(-1, 1, size=size)\n",
    "    \n",
    "class HeuristicPendulumAgent(Agent):\n",
    "    def __init__(self, env: NormalizedEnv, torque_intensity: float=1.0) -> None:\n",
    "        super().__init__(env)\n",
    "        self.torque_intensity = torque_intensity\n",
    "        \n",
    "    def compute_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the action to apply to the environment.\n",
    "\n",
    "        When the pendulum is in the lower half of the domain, applies a fixed torque in the same direction as the pendulum's angular velocity.\n",
    "        When the pendulum is in the upper half of the domain, applies a fixed torque in the opposite direction as the pendulum's angular velocity.\n",
    "        \"\"\"\n",
    "        action = np.where(state[..., [0]] < 0, # If the pendulum is in the lower half of the circle\n",
    "                        np.sign(state[..., [2]]) * self.torque_intensity, \n",
    "                        -np.sign(state[..., [2]]) * self.torque_intensity)\n",
    "        return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_return_mean_std(agent: Agent, num_episodes: int = 10, env: NormalizedEnv = env) -> float:\n",
    "    \"\"\"Run the agent for `num_episodes` episodes and return the average return and its standard deviation.\"\"\"\n",
    "    returns = np.empty(num_episodes)\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        cumulative_reward = 0\n",
    "        while True:\n",
    "            # Simulation step\n",
    "            action = agent.compute_action(state)\n",
    "            state, reward, _, trunc, _ = env.step(action)\n",
    "            cumulative_reward += reward\n",
    "            if trunc:\n",
    "                break\n",
    "        returns[episode] = cumulative_reward\n",
    "    return np.mean(returns), np.std(returns)\n",
    "\n",
    "def animation(filename: str, agent: Agent, env: NormalizedEnv = env) -> None:\n",
    "    \"\"\"Run the agent for one episode and save the frames as a gif.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    frames = []\n",
    "    while True:\n",
    "        action = agent.compute_action(state)\n",
    "        state, _, _, trunc, _ = env.step(action)\n",
    "        frames.append(env.render())\n",
    "        if trunc:\n",
    "            break\n",
    "\n",
    "    frames = np.array(frames)\n",
    "    frames = [Image.fromarray(frame) for frame in frames]\n",
    "    frames[0].save(filename + \".gif\", save_all=True, append_images=frames[1:], duration=50, loop=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"Buffer to store transitions.\n",
    "\n",
    "        A transition is a tuple (state, action, reward, next_state, trunc) where:\n",
    "            state (np.ndarray[(3,), np.float32]): State of the environment.\n",
    "            action (np.ndarray[(1,), np.float32]): Action applied to the environment.\n",
    "            reward (np.ndarray[(1,), np.float32]): Reward obtained after applying the action.\n",
    "            next_state (np.ndarray[(3,), np.float32]): State of the environment after applying the action.\n",
    "            trunc (np.ndarray[(1,), np.bool]): Boolean indicating if the episode is truncated.\n",
    "        \n",
    "        The buffer is implemented as 5 (one for each element of a transition) cyclic numpy arrays of shape \n",
    "        (capacity, *) where * is the shape of the corresponding element of a transition.\n",
    "        When the buffer is full, the oldest transitions are dropped.\n",
    "        \n",
    "        Args:\n",
    "            capacity (int): Capacity of the buffer.\n",
    "        \"\"\"\n",
    "        self.capacity = capacity \n",
    "        self.states = np.empty((capacity, 3), np.float32)\n",
    "        self.actions = np.empty((capacity, 1), np.float32)\n",
    "        self.rewards = np.empty((capacity, 1), np.float32)\n",
    "        self.next_states = np.empty((capacity, 3), np.float32)\n",
    "        self.truncs = np.empty((capacity, 1), bool)\n",
    "        # Index of the next transition to be stored\n",
    "        self.index = 0\n",
    "        # Current size of the buffer\n",
    "        self.size = 0\n",
    "\n",
    "    def append(self, transition):\n",
    "        \"\"\"Add a transition to the buffer.\n",
    "        \n",
    "        See constructor docstring for explanation of a transition.\n",
    "        \"\"\"\n",
    "        # Add transition to current index\n",
    "        self.states[self.index] = transition[0]\n",
    "        self.actions[self.index] = transition[1]\n",
    "        self.rewards[self.index] = transition[2]\n",
    "        self.next_states[self.index] = transition[3]\n",
    "        self.truncs[self.index] = transition[4]\n",
    "        # Update index\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of size `batch_size` of transitions from the buffers.\"\"\"\n",
    "        indexes = np.random.choice(self.size, batch_size, replace=False)\n",
    "        return self[indexes]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, slice):\n",
    "        return (self.states[slice], \n",
    "                self.actions[slice], \n",
    "                self.rewards[slice], \n",
    "                self.next_states[slice], \n",
    "                self.truncs[slice])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks and Learning Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int=env.observation_space.shape[0], action_size: int=env.action_space.shape[0]):\n",
    "        \"\"\"QNetwork. Maps (state, action) pairs to Q-values.\"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): State tensor\n",
    "            action (torch.Tensor): Action tensor\n",
    "\n",
    "        Returns:\n",
    "            q_value (torch.Tensor): Q-value tensor\n",
    "        \"\"\"\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def TD_learning_update(q_network: QNetwork, \n",
    "                       agent: Agent, \n",
    "                       optimizer: torch.optim.Optimizer, \n",
    "                       transitions: tuple, \n",
    "                       gamma: float) -> float:\n",
    "    \"\"\"Perform a 1-step TD-learning update for a batch of transitions.\"\"\"\n",
    "    states = transitions[0]\n",
    "    actions = transitions[1]\n",
    "    rewards = transitions[2]\n",
    "    next_states = transitions[3]\n",
    "    truncs = transitions[4]\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    states_t = torch.from_numpy(states).to(device)\n",
    "    actions_t = torch.from_numpy(actions).to(device)\n",
    "    rewards_t = torch.from_numpy(rewards).to(device)\n",
    "    next_states_t = torch.from_numpy(next_states).to(device)\n",
    "    truncs_t = torch.from_numpy(truncs).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_actions = agent.compute_action(next_states)\n",
    "        next_actions_t = torch.as_tensor(next_actions).to(device, dtype=torch.float32)\n",
    "        # Naive next Q-values (i.e. without taking truncation into account)\n",
    "        naive_next_qs = q_network(next_states_t, next_actions_t)\n",
    "        # Actual next Q-values (i.e. taking truncation into account)\n",
    "        next_qs = torch.where(truncs_t, \n",
    "                              torch.zeros_like(rewards_t), \n",
    "                              naive_next_qs)\n",
    "        gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "        targets = rewards_t + gamma * next_qs\n",
    "\n",
    "    loss = F.mse_loss(q_network(states_t, actions_t), targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: Agent,\n",
    "          q_network: QNetwork, \n",
    "          q_optimizer: torch.optim.Optimizer,\n",
    "          n_episodes: int=1000, \n",
    "          batch_size: int=128, \n",
    "          gamma: float=0.99, \n",
    "          logging_interval: int=10, \n",
    "          capacity: int=10000, \n",
    "          env: NormalizedEnv=env,\n",
    "          n_steps: int=n_steps, \n",
    "          verbose: bool=True) -> np.ndarray:\n",
    "    \"\"\"Train the DDPG agent.\n",
    "\n",
    "    Args:\n",
    "        agent (Agent): Agent to train\n",
    "        q_network (QNetwork): Q-network\n",
    "        q_optimizer (torch.optim.Optimizer): Optimizer for the Q-network\n",
    "        n_episodes (int, optional): Number of episodes to train for. Defaults to 1000.\n",
    "        batch_size (int, optional): Batch size for training. Defaults to 128.\n",
    "        gamma (float, optional): Discount factor. Defaults to 0.99.\n",
    "        logging_interval (int, optional): Logging interval. Defaults to 10.\n",
    "        capacity (int, optional): Capacity of the replay buffer. Defaults to 10000.\n",
    "        env (NormalizedEnv, optional): Environment. Defaults to env.\n",
    "        n_steps (int, optional): Number of steps per episode. Defaults to n_steps.\n",
    "        verbose (bool, optional): Whether to print training logs. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        losses (np.ndarray): Losses\n",
    "    \"\"\"\n",
    "    replay_buffer = ReplayBuffer(capacity=capacity)\n",
    "    losses = np.empty((n_episodes, n_steps))\n",
    "    progress_bar = tqdm(range(n_episodes), desc=\"Training\", disable=not verbose, unit=\"episode\")\n",
    "    with progress_bar as episodes:\n",
    "        for episode in episodes:\n",
    "            state, _ = env.reset()\n",
    "            step = 0\n",
    "            while True:\n",
    "                # Simulation step\n",
    "                action = agent.compute_action(state)\n",
    "                next_state, reward, _, trunc, _ = env.step(action)\n",
    "                transition = (state, action, reward, next_state, trunc)\n",
    "                replay_buffer.append(transition)\n",
    "                # Training step\n",
    "                if len(replay_buffer) >= batch_size:\n",
    "                    transitions = replay_buffer.sample(batch_size)\n",
    "                    loss = TD_learning_update(q_network, agent, q_optimizer, transitions, gamma)\n",
    "                    losses[episode, step] = loss\n",
    "                else:\n",
    "                    losses[episode, step] = np.nan\n",
    "\n",
    "                if trunc:\n",
    "                    break\n",
    "                state = next_state\n",
    "                step += 1    \n",
    "            episodes.set_postfix_str(f\"Loss: {np.nanmean(losses[episode])}\")\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomAgent(env)\n",
    "average_return, _ = compute_return_mean_std(random_agent, 10, env)\n",
    "print('Average return over 10 episodes: {}'.format(average_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best torque intensity\n",
    "torque_intensities = np.linspace(0.1, 1, 10)\n",
    "average_returns = np.empty(len(torque_intensities))\n",
    "for i, torque_intensity in enumerate(torque_intensities):\n",
    "    heuristic_agent = HeuristicPendulumAgent(env, torque_intensity=torque_intensity)\n",
    "    average_returns[i], _ = compute_return_mean_std(heuristic_agent, 10, env)\n",
    "\n",
    "plt.plot(torque_intensities, average_returns)\n",
    "plt.xlabel('Torque intensity')\n",
    "plt.ylabel('Average return over 10 episodes')\n",
    "plt.show()\n",
    "\n",
    "# Create a HeuristicPendulumAgent with the best torque intensity\n",
    "heuristic_agent = HeuristicPendulumAgent(env, torque_intensity=torque_intensities[np.argmax(average_returns)])\n",
    "# Print best reward over 10 episodes and associated torque intensity\n",
    "print('Best torque intensity: {}'.format(torque_intensities[np.argmax(average_returns)]))\n",
    "print('Best return over 10 episodes: {}'.format(np.max(average_returns)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment consists of a pendulum attached to a fixed pivot point. Its position is described by the angle $\\alpha$ between the pendulum and the upward vertical direction, and its (angular) velocity is $\\dot{\\alpha}$. The pendulum is actuated by applying a torque $\\tau$ on its pivot point (the action space is thus continuous). The pendulum is subject to gravity, which is the only external force acting on it.\n",
    "\n",
    "The pendulum starts in a random position, with a random velocity and the aim is to stabilize it in the inverted position, using little torque. More precisely, we want to maximise cummulative reward $-\\left(\\alpha^2 + 0.1\\dot{\\alpha}^2 + 0.001\\tau^2\\right)$ over a time horizon of 200 steps. The pendulum starts in a random position, with a random velocity.\n",
    "\n",
    "First we will compare two policies: a random policy and a heuristic policy. \n",
    "The random policy is a policy that selects actions uniformly at random from the action space, i.e. it applies a tork of random magnitude (within the allowed range)in a random direction.\n",
    "The heuristic policy is defined as follows:\n",
    "- When the pendulum is in the lower half of the domain, applies a fixed torque in the same direction as the pendulum's angular velocity;\n",
    "- When the pendulum is in the upper half of the domain, applies a fixed torque in the opposite direction as the pendulum's angular velocity.\n",
    "\n",
    "The average return over 10 episodes for the random policy is $\\sim (-1300)$, which sets a baseline for the performance of the policies.\n",
    "\n",
    "On Fig. 1, we see the average return over 10 episodes for the heuristic policy depending on the magnitude of the fixed torque. We can see that the heuristic policy performs better than the random policy for all values of the fixed torque, starting from $\\sim (-900)$ for a fixed torque of $0.1$, then increasing to $\\sim (-500)$ for a fixed torque of $\\sim 0.5$, and then plateauing around $\\sim (-450)$ up to a fixed torque of $1$. The best performance lies on this plateau but its exact value varies from one run to another. TODO check the values depending on the seed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Q function of the heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = QNetwork(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0]).to(device)\n",
    "q_network_before_training = copy.deepcopy(q_network)\n",
    "optimizer = torch.optim.SGD(q_network.parameters(), lr=1e-4)\n",
    "losses = train(heuristic_agent, \n",
    "               q_network, \n",
    "               optimizer, \n",
    "               n_episodes=1000, \n",
    "               batch_size=128, \n",
    "               gamma=0.99, \n",
    "               logging_interval=10, \n",
    "               capacity=10000, \n",
    "               env=env, \n",
    "               n_steps=n_steps, \n",
    "               verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean loss per episode\n",
    "plt.plot(np.nanmean(losses, axis=1))\n",
    "# Add linear regression line\n",
    "x = np.arange(losses.shape[0])\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, np.nanmean(losses, axis=1))\n",
    "plt.plot(x, slope*x + intercept, 'r--')\n",
    "\n",
    "# Add legend and labels\n",
    "plt.legend(['Mean loss', 'Linear regression, slope: {:.2f}'.format(slope)])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = -1\n",
    "velocity = 1\n",
    "\n",
    "rad = np.linspace(0, 5, 100)\n",
    "azm = np.linspace(0, 2 * np.pi, 100)\n",
    "r, th = np.meshgrid(rad, azm)\n",
    "states = np.empty([100, 100, 3])\n",
    "states[...,0] = np.cos(th)\n",
    "states[...,1] = np.sin(th)\n",
    "states[...,2] = np.full((100,100), velocity)\n",
    "\n",
    "actions = np.full((100,100, 1), action)\n",
    "\n",
    "\n",
    "#before training\n",
    "z = q_network_before_training(torch.Tensor(states), torch.Tensor(actions)).detach().squeeze()\n",
    "\n",
    "print(np.array(z).shape)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "rad = np.linspace(0, 5, 100)\n",
    "azm = np.linspace(0, 2 * np.pi, 100)\n",
    "r, th = np.meshgrid(rad, azm)\n",
    "\n",
    "ax = plt.subplot(projection=\"polar\")\n",
    "ax.set_theta_zero_location('N')\n",
    "\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "p = plt.pcolormesh(th, r, z)\n",
    "fig.colorbar(p)\n",
    "\n",
    "plt.plot(azm, r, color='k', ls='none') \n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#after training\n",
    "z = q_network(torch.Tensor(states), torch.Tensor(actions)).detach().squeeze()\n",
    "\n",
    "print(np.array(z).shape)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "rad = np.linspace(0, 5, 100)\n",
    "azm = np.linspace(0, 2 * np.pi, 100)\n",
    "r, th = np.meshgrid(rad, azm)\n",
    "\n",
    "ax = plt.subplot(projection=\"polar\")\n",
    "ax.set_theta_zero_location('N')\n",
    "\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "p = plt.pcolormesh(th, r, z)\n",
    "fig.colorbar(p)\n",
    "\n",
    "plt.plot(azm, r, color='k', ls='none') \n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement a critic, i.e. a Q-network, whose goal is to estimate the Q-function of the heuristic policy. We use a neural network with two hidden layers of 32 neurons each, with ReLU activation functions. It is updated using the semi-gradient of the mean squared error between the predicted and target Q-values.\n",
    "\n",
    "On Fig. 2 we see the training curve with batches of size 128 and 1000 epochs. We can see that the training is very fast, with the loss decreasing to a value of $\\sim 0.1$ after only 100 epochs. The loss then decreases very slowly, reaching a value of $\\sim 0.05$ after 1000 epochs. This is due to the fact that the heuristic policy is deterministic, so the Q-values are constant and the critic can easily learn them.\n",
    "\n",
    " The training is done using the data generated by the heuristic policy, i.e. the states and the corresponding Q-values. The training is done over 1000 epochs, with a batch size of 64."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Minimal implementation of DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int=3, action_size: int=1):\n",
    "        \"\"\"PolicyNetwork. Maps states to actions.\"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): State tensor\n",
    "\n",
    "        Returns:\n",
    "            action (torch.Tensor): Action tensor\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianActionNoise:\n",
    "    def __init__(self, sigma: float=0.3):\n",
    "        assert(sigma>0), 'sigma must be positive'\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def get_noisy_action(self, action: torch.Tensor):\n",
    "        noise = torch.normal(mean=0, std=self.sigma, size=action.shape)\n",
    "        return torch.clamp(action + noise, -1, +1)\n",
    "    \n",
    "class OUActionNoise:\n",
    "    def __init__(self, theta: float=0.15, sigma: float=0.3):\n",
    "        assert(theta>=0 and theta<=1), 'theta must be between 0 and 1 (included)'\n",
    "        assert(sigma>=0), 'sigma must non-negative'\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.noise = torch.zeros(env.action_space.shape[0])\n",
    "        \n",
    "    def get_noisy_action(self, action: torch.Tensor):\n",
    "        self.noise = (1-self.theta)*self.noise + torch.normal(0, self.sigma, size=self.noise.shape)\n",
    "        return torch.clamp(action + self.noise, -1, +1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, action_noise: 'ActionNoise'=GaussianActionNoise()):\n",
    "        self.policy_network = PolicyNetwork().to(device)\n",
    "        self.noise = action_noise\n",
    "        \n",
    "    def compute_action(self, state, deterministic=True):\n",
    "        state_t = torch.from_numpy(state).to(device)\n",
    "        action = self.policy_network(state_t)\n",
    "        if not deterministic:\n",
    "            action = self.noise.get_noisy_action(action)\n",
    "        return action\n",
    "    \n",
    "def policy_learning_update(agent, optimizer, transitions, deterministic: bool=True):\n",
    "    states = transitions[0]\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    states_t = torch.from_numpy(states).to(device)\n",
    "    \n",
    "    \n",
    "    actions_t = agent.compute_action(states, deterministic)\n",
    "\n",
    "    # Next Q-values (without taking truncation ??)\n",
    "    qs = q_network(states_t, actions_t)\n",
    "\n",
    "    loss = -qs.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpg(agent, q_network, optimizer_q, optimizer_policy, n_episodes: int=1000, \n",
    "               batch_size: int=128, gamma: float=0.99, logging_interval: int=10, \n",
    "               capacity: int=10000):\n",
    "    \"\"\"Train the DDPG agent.\n",
    "\n",
    "    Args:\n",
    "        agent (DDPGAgent): The agent to train.\n",
    "        q_network (QNetwork): The Q-network to train.\n",
    "        optimizer_q (torch.optim.Optimizer): The optimizer for the Q-network.\n",
    "        optimizer_policy (torch.optim.Optimizer): The optimizer for the policy network.\n",
    "        n_episodes (int, optional): Number of episodes to train. Defaults to 1000.\n",
    "        batch_size (int, optional): Batch size for the TD-learning update. Defaults to 128.\n",
    "        gamma (float, optional): Discount factor. Defaults to 0.99.\n",
    "        logging_interval (int, optional): Logging interval. Defaults to 10.\n",
    "        capacity (int, optional): Capacity of the replay buffer. Defaults to 10000.\n",
    "    \n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray, np.ndarray): The losses of the Q-network, the losses of the policy network and the episode rewards.\n",
    "    \"\"\"\n",
    "    # Number of steps per episode, see documentation of the Pendulum-v1 environment\n",
    "    n_steps = 200\n",
    "    replay_buffer = ReplayBuffer(capacity=capacity)\n",
    "    # Container for the loss of each step of each episode\n",
    "    losses_q = np.empty((n_episodes, n_steps))\n",
    "    losses_policy = np.empty((n_episodes, n_steps))\n",
    "    episode_rewards = np.empty(n_episodes)\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        step = 0\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            # Simulation step\n",
    "            action = agent.compute_action(state, deterministic=False).detach().numpy()\n",
    "            next_state, reward, _, trunc, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            replay_buffer.append((state, action, reward, next_state, trunc))\n",
    "            # Training step\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                transitions = replay_buffer.sample(batch_size)\n",
    "                loss_q = TD_learning_update(q_network, agent, optimizer_q, transitions, gamma)\n",
    "                losses_q[episode, step] = loss_q.item()\n",
    "                # TODO ask deterministic or not\n",
    "                loss_policy = policy_learning_update(agent, optimizer_policy, transitions, deterministic=False)\n",
    "                losses_policy[episode, step] = loss_policy.item()\n",
    "            else:\n",
    "                losses_q[episode, step] = np.nan\n",
    "                losses_policy[episode, step] = np.nan\n",
    "\n",
    "            if trunc:\n",
    "                break\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        episode_rewards[episode] = episode_reward\n",
    "        # Logging average loss of the last episode every `logging_interval` episodes\n",
    "        if episode % logging_interval == 0 and episode > 0:\n",
    "            print(f'Episode {episode}/{n_episodes-1}, average loss q-network: {np.mean(losses_q[episode])}, average loss policy network: {np.mean(losses_policy[episode])}')\n",
    "\n",
    "    return losses_q, losses_policy, episode_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = QNetwork(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0]).to(device)\n",
    "q_network_before_training = copy.deepcopy(q_network)\n",
    "action_noise = GaussianActionNoise(sigma=0.3)\n",
    "agent = DDPGAgent(action_noise=action_noise)\n",
    "optimizer_q = torch.optim.SGD(q_network.parameters(), lr=1e-4)\n",
    "optimizer_policy = torch.optim.SGD(agent.policy_network.parameters(), lr=1e-4)\n",
    "\n",
    "losses_q, losses_policy, episode_rewards = train_ddpg(\n",
    "        agent, q_network, optimizer_q, optimizer_policy, n_episodes=100, batch_size=128, \n",
    "        gamma=0.99, capacity=10000, logging_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean loss of q network per episode\n",
    "plt.plot(np.nanmean(losses_q, axis=1))\n",
    "\n",
    "# legend\n",
    "plt.legend(['Mean loss q network'])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean loss of policy network per episode\n",
    "plt.plot(np.nanmean(losses_policy, axis=1))\n",
    "\n",
    "# legend\n",
    "plt.legend(['Mean loss policy network'])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the episode reward for each episode\n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "# legend\n",
    "plt.legend(['Episode reward'])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "episode_rewards = np.empty(n_episodes)\n",
    "for episode in range(n_episodes):\n",
    "    episode_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    while True:\n",
    "        action = agent.compute_action(state, deterministic=False).detach().numpy()\n",
    "        next_state, reward, _, trunc, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if trunc:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    episode_rewards[episode] = episode_reward\n",
    "\n",
    "print('Average reward over 100 episodes: {}'.format(np.mean(episode_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = -1\n",
    "velocity = 2\n",
    "\n",
    "rad = np.linspace(0, 5, 100)\n",
    "azm = np.linspace(0, 2 * np.pi, 100)\n",
    "r, th = np.meshgrid(rad, azm)\n",
    "states = np.empty([100, 100, 3])\n",
    "states[...,0] = np.cos(th)\n",
    "states[...,1] = np.sin(th)\n",
    "states[...,2] = np.full((100,100), velocity)\n",
    "\n",
    "actions = np.full((100,100, 1), action)\n",
    "\n",
    "\n",
    "#before training\n",
    "z = q_network_before_training(torch.Tensor(states), torch.Tensor(actions)).detach().squeeze()\n",
    "\n",
    "print(np.array(z).shape)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "rad = np.linspace(0, 5, 100)\n",
    "azm = np.linspace(0, 2 * np.pi, 100)\n",
    "r, th = np.meshgrid(rad, azm)\n",
    "\n",
    "ax = plt.subplot(projection=\"polar\")\n",
    "ax.set_theta_zero_location('N')\n",
    "\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "p = plt.pcolormesh(th, r, z)\n",
    "fig.colorbar(p)\n",
    "\n",
    "plt.plot(azm, r, color='k', ls='none') \n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#after training\n",
    "z = q_network(torch.Tensor(states), torch.Tensor(actions)).detach().squeeze()\n",
    "\n",
    "print(np.array(z).shape)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "rad = np.linspace(0, 5, 100)\n",
    "azm = np.linspace(0, 2 * np.pi, 100)\n",
    "r, th = np.meshgrid(rad, azm)\n",
    "\n",
    "ax = plt.subplot(projection=\"polar\")\n",
    "ax.set_theta_zero_location('N')\n",
    "\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "p = plt.pcolormesh(th, r, z)\n",
    "fig.colorbar(p)\n",
    "\n",
    "plt.plot(azm, r, color='k', ls='none') \n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = replay_buffer.sample(batch_size)\n",
    "velocities = transitions[0][:,2]\n",
    "print(velocities.max())\n",
    "#save model and losses\n",
    "\"\"\"\n",
    "torch.save(q_network.state_dict(), \"5_q_network.pt\")\n",
    "torch.save(agent.policy_network.state_dict(), \"5_policy_network.pt\")\n",
    "np.save(\"5_q_loss.txt\", losses_q)\n",
    "np.save(\"5_policy_loss.txt\", losses_policy)\n",
    "np.save(\"5_rewards.txt\", episode_rewards)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgentWithTargetNetwork:\n",
    "    def __init__(self, action_noise: 'ActionNoise'=GaussianActionNoise()):\n",
    "        self.policy_network = PolicyNetwork().to(device)\n",
    "        self.target_policy_network = copy.deepcopy(self.policy_network).to(device)\n",
    "        self.target_policy_network = PolicyNetwork().to(device)\n",
    "        self.target_policy_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.noise = action_noise\n",
    "        \n",
    "    def compute_action(self, state, target=False, deterministic=True):\n",
    "        state_t = torch.from_numpy(state).to(device)\n",
    "        if target:\n",
    "            action = self.target_policy_network(state_t)\n",
    "        else:\n",
    "            action = self.policy_network(state_t)\n",
    "        if not deterministic:\n",
    "            action = self.noise.get_noisy_action(action)\n",
    "        return action\n",
    "\n",
    "def policy_learning_update_with_target(agent, optimizer, transitions, deterministic: bool=True):\n",
    "    states = transitions[0]\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    states_t = torch.from_numpy(states).to(device)\n",
    "    \n",
    "    \n",
    "    actions_t = agent.compute_action(states, target=False, deterministic=deterministic)\n",
    "\n",
    "    # Next Q-values (without taking truncation ??)\n",
    "    qs = q_network(states_t, actions_t)\n",
    "\n",
    "    loss = -qs.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_learning_update_with_target_networks(q_network, target_q_network, agent, optimizer, transitions, gamma):\n",
    "    \"\"\"Perform a 1-step TD-learning update for a batch of transitions.\n",
    "\n",
    "    Args:\n",
    "        q_network (QNetwork): QNetwork instance\n",
    "        agent (Agent): Agent (i.e. Policy) instance\n",
    "        optimizer (torch.optim): Optimizer instance\n",
    "        transitions (List[Transition]): Batch of transitions\n",
    "        gamma (float): Discount factor\n",
    "\n",
    "    Returns:\n",
    "        loss (float): Loss value\n",
    "    \"\"\"\n",
    "    states = transitions[0]\n",
    "    actions = transitions[1]\n",
    "    rewards = transitions[2]\n",
    "    next_states = transitions[3]\n",
    "    truncs = transitions[4]\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    states_t = torch.from_numpy(states).to(device)\n",
    "    actions_t = torch.from_numpy(actions).to(device)\n",
    "    rewards_t = torch.from_numpy(rewards).to(device)\n",
    "    next_states_t = torch.from_numpy(next_states).to(device)\n",
    "    truncs_t = torch.from_numpy(truncs).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_actions = agent.compute_action(next_states, target=True, deterministic=True)\n",
    "        if not torch.is_tensor(next_actions):\n",
    "            next_actions_t = torch.as_tensor(next_actions)[:, None].to(device, dtype=torch.float32)\n",
    "        else: next_actions_t = next_actions\n",
    "        # Naive next Q-values (i.e. without taking truncation into account)\n",
    "        naive_next_qs = target_q_network(next_states_t, next_actions_t)\n",
    "        # Actual next Q-values (i.e. taking truncation into account)\n",
    "        next_qs = torch.where(truncs_t, \n",
    "                              torch.zeros_like(rewards_t), \n",
    "                              naive_next_qs)\n",
    "        gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "        targets = rewards_t + gamma * next_qs\n",
    "\n",
    "    loss = F.mse_loss(q_network(states_t, actions_t), targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_learning(network, target_network, tau):\n",
    "    target_network_state_dict = target_network.state_dict()\n",
    "    network_state_dict = network.state_dict()\n",
    "    for key in network_state_dict:\n",
    "        target_network_state_dict[key] = network_state_dict[key]*tau + target_network_state_dict[key]*(1-tau)\n",
    "    target_network.load_state_dict(target_network_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpg_with_target(agent, q_network, target_q_network, optimizer_q, optimizer_policy, n_episodes: int=1000, \n",
    "               batch_size: int=128, gamma: float=0.99, logging_interval: int=10, \n",
    "               capacity: int=10000, tau: float=0.05):\n",
    "    \n",
    "    # Number of steps per episode, see documentation of the Pendulum-v1 environment\n",
    "    n_steps = 200\n",
    "    replay_buffer = ReplayBuffer(capacity=capacity)\n",
    "    # Container for the loss of each step of each episode\n",
    "    losses_q = np.empty((n_episodes, n_steps))\n",
    "    losses_policy = np.empty((n_episodes, n_steps))\n",
    "    episode_rewards = np.empty(n_episodes)\n",
    "    logging_interval = 10\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        step = 0\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            # Simulation step\n",
    "            action = agent.compute_action(state, target=False, deterministic=False).detach().numpy()\n",
    "            next_state, reward, _, trunc, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            replay_buffer.append((state, action, reward, next_state, trunc))\n",
    "            # Training step\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                transitions = replay_buffer.sample(batch_size)\n",
    "                loss_q = TD_learning_update_with_target_networks(q_network, target_q_network, agent, optimizer_q, transitions, gamma)\n",
    "                losses_q[episode, step] = loss_q.item()\n",
    "                loss_policy = policy_learning_update_with_target(agent, optimizer_policy, transitions, deterministic=True)\n",
    "                losses_policy[episode, step] = loss_policy.item()\n",
    "\n",
    "                # Update of the target networks\n",
    "                target_learning(q_network, target_q_network, tau=tau)\n",
    "                target_learning(agent.policy_network, agent.target_policy_network, tau=tau)\n",
    "            else:\n",
    "                losses_q[episode, step] = np.nan\n",
    "                losses_policy[episode, step] = np.nan\n",
    "\n",
    "            if trunc:\n",
    "                break\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        episode_rewards[episode] = episode_reward\n",
    "        # Logging average loss of the last episode every `logging_interval` episodes\n",
    "        if episode % logging_interval == 0 and episode > 0:\n",
    "            print(f'Episode {episode}/{n_episodes-1}, average loss q-network: {np.mean(losses_q[episode])}, average loss policy network: {np.mean(losses_policy[episode])}')\n",
    "    return losses_q, losses_policy, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = QNetwork(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0]).to(device)\n",
    "target_q_network = QNetwork(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0]).to(device)\n",
    "target_q_network.load_state_dict(q_network.state_dict())\n",
    "action_noise = GaussianActionNoise(sigma=0.3)\n",
    "agent = DDPGAgentWithTargetNetwork(action_noise=action_noise)\n",
    "optimizer_q = torch.optim.SGD(q_network.parameters(), lr=1e-4)\n",
    "optimizer_policy = torch.optim.SGD(agent.policy_network.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop for the Q-learning\n",
    "n_episodes = 400 # Number of episodes\n",
    "\n",
    "losses_q, losses_policy, episode_rewards = train_ddpg_with_target(agent, q_network, target_q_network, optimizer_q, optimizer_policy,\n",
    "                                                      n_episodes=n_episodes, batch_size=128, gamma=0.99, logging_interval=10,\n",
    "                                                      capacity=10000, tau=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean loss of q network per episode\n",
    "plt.plot(np.nanmean(losses_q, axis=1))\n",
    "\n",
    "# legend\n",
    "plt.legend(['Mean loss q network'])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean loss of policy network per episode\n",
    "plt.plot(np.nanmean(losses_policy, axis=1))\n",
    "\n",
    "# legend\n",
    "plt.legend(['Mean loss policy network'])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean loss')\n",
    "plt.show()\n",
    "print(losses_policy[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the episode reward for each episode\n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "# legend\n",
    "plt.legend(['Episode reward'])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "episode_rewards = np.empty(n_episodes)\n",
    "for episode in range(n_episodes):\n",
    "    episode_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    while True:\n",
    "        action = agent.compute_action(state, target=False, deterministic=False).detach().numpy()\n",
    "        next_state, reward, _, trunc, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if trunc:\n",
    "            break\n",
    "        state = next_state\n",
    "        episode_rewards[episode] = episode_reward\n",
    "        \n",
    "        print('Average reward over 100 episodes: {}'.format(np.mean(episode_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation of the Heuristic Agent using PIL\n",
    "\"\"\"\n",
    "state, _ = env.reset()\n",
    "frames = []\n",
    "while True:\n",
    "    action = agent.compute_action(state, target=False, deterministic=True).detach().numpy()\n",
    "    state, reward, _, trunc, _ = env.step(action)\n",
    "    frames.append(env.render())\n",
    "    if trunc:\n",
    "        break\n",
    "\n",
    "frames = np.array(frames)\n",
    "frames = [Image.fromarray(frame) for frame in frames]\n",
    "frames[0].save(\"agent.gif\", save_all=True, append_images=frames[1:], duration=50, loop=0)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Ornstein-Uhlenbeck noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = [0, 0.25, 0.5, 0.75, 1]\n",
    "for theta in thetas:\n",
    "    q_network = QNetwork().to(device)\n",
    "    action_noise = OUActionNoise(theta=theta, sigma=0.3)\n",
    "    agent = DDPGAgent(action_noise=action_noise)\n",
    "    optimizer_q = torch.optim.SGD(q_network.parameters(), lr=1e-4)\n",
    "    optimizer_policy = torch.optim.SGD(agent.policy_network.parameters(), lr=1e-4)\n",
    "    losses_q, losses_policy, episode_rewards = train_ddpg_with_target(\n",
    "        agent, q_network, optimizer_q, optimizer_policy, n_episodes=100, batch_size=128, \n",
    "        gamma=0.99, capacity=10000, logging_interval=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
