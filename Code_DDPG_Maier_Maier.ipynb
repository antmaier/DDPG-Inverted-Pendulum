{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project: Control in a Continuous Action Space with DDPG\n",
    "\n",
    "In this project, we will explore the Deep Deterministic Policy Gradient (DDPG) algorithm, which is designed to handle continuous action spaces in Reinforcement Learning. We will use the Pendulum-v1 environment implemented in OpenAI Gym to implement the DDPG algorithm from scratch to solve the classical control problem of stabilizing an inverted pendulum. Throughout the development, we will incrementally build the components of DDPG and analyze their importance for correct and effective learning. This Jupyter notebook contains our implementation and report for this project. \n",
    "\n",
    "The instructions are available in `Miniproject_DDPG.pdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from helpers import NormalizedEnv\n",
    "from helpers import RandomAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device {device} is available\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environement instance, an instance of the Pendulum-v1 environment wrapped in a NormalizedEnv class\n",
    "# to normalize the action space between -1 and 1\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "env = NormalizedEnv(env)\n",
    "\n",
    "# Create a RandomAgent\n",
    "random_agent = RandomAgent(env)\n",
    "\n",
    "def run_agent(agent: 'Agent', env: NormalizedEnv, num_episodes: int) -> float:\n",
    "    \"\"\"Run the agent for `num_episodes` episodes and return the average reward\"\"\"\n",
    "    rewards = [] # Will contain the reward of each step\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            # Simulation step\n",
    "            action = agent.compute_action(state)\n",
    "            state, reward, _, trunc, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if trunc:\n",
    "                break\n",
    "        rewards.append(episode_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "average_reward = run_agent(random_agent, env, 10)\n",
    "print('Average reward over 10 episodes: {}'.format(average_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeuristicPendulumAgent:\n",
    "    def __init__(self, env: NormalizedEnv, torque_intensity: float=1.0) -> None:\n",
    "        \"\"\"Heuristic agent for the Pendulum-v1 environment.\"\"\"\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.torque_intensity = torque_intensity\n",
    "        \n",
    "    def compute_action(self, state):\n",
    "        \"\"\"Compute the action to apply to the environment.\n",
    "\n",
    "        When the pendulum is in the lower half of the domain, applies a fixed torque in the same direction as the pendulum's angular velocity.\n",
    "        When the pendulum is in the upper half of the domain, applies a fixed torque in the opposite direction as the pendulum's angular velocity.\n",
    "        \"\"\"\n",
    "        action = np.where(state[..., 0] < 0, # If the pendulum is in the lower half of the circle\n",
    "                        np.sign(state[..., 2]) * self.torque_intensity, \n",
    "                        -np.sign(state[..., 2]) * self.torque_intensity)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best torque intensity\n",
    "torque_intensities = np.linspace(0.1, 1, 10)\n",
    "rewards = []\n",
    "for torque_intensity in torque_intensities:\n",
    "    heuristic_agent = HeuristicPendulumAgent(env, torque_intensity=torque_intensity)\n",
    "    rewards.append(run_agent(heuristic_agent, env, 10))\n",
    "\n",
    "plt.plot(torque_intensities, rewards)\n",
    "plt.xlabel('Torque intensity')\n",
    "plt.ylabel('Average reward over 10 episodes')\n",
    "plt.show()\n",
    "\n",
    "# Create a HeuristicPendulumAgent with the best torque intensity\n",
    "heuristic_agent = HeuristicPendulumAgent(env, torque_intensity=torque_intensities[np.argmax(rewards)])\n",
    "# Print best reward over 10 episodes and associated torque intensity\n",
    "print('Best torque intensity: {}'.format(torque_intensities[np.argmax(rewards)]))\n",
    "print('Best reward over 10 episodes: {}'.format(np.max(rewards)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation of the Heuristic Agent using PIL\n",
    "state, _ = env.reset()\n",
    "frames = []\n",
    "while True:\n",
    "    action = heuristic_agent.compute_action(state)\n",
    "    state, reward, _, trunc, _ = env.step(action)\n",
    "    frames.append(env.render())\n",
    "    if trunc:\n",
    "        break\n",
    "\n",
    "frames = np.array(frames)\n",
    "frames = [Image.fromarray(frame) for frame in frames]\n",
    "frames[0].save(\"heuristic_agent.gif\", save_all=True, append_images=frames[1:], duration=50, loop=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q function of the heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_type = np.dtype([('state', np.float32, (3,)),\n",
    "                            ('action', np.float32, (1,)),\n",
    "                            ('reward', np.float32, (1,)),\n",
    "                            ('next_state', np.float32, (3,)),\n",
    "                            ('trunc', bool, (1,))],\n",
    "                            align=True)\n",
    "\n",
    "# test transition type from simulation\n",
    "state, _ = env.reset()\n",
    "action = heuristic_agent.compute_action(state)\n",
    "next_state, reward, _, trunc, _ = env.step(action)\n",
    "transition = (state, action, reward, next_state, trunc)\n",
    "# Do a 2nd step\n",
    "state = next_state\n",
    "action = heuristic_agent.compute_action(state)\n",
    "next_state, reward, _, trunc, _ = env.step(action)\n",
    "transition = (state, action, reward, next_state, trunc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"Buffer to store transitions.\n",
    "\n",
    "        A transition is a tuple (state, action, reward, next_state, trunc) where:\n",
    "            state (np.ndarray[(3,), np.float32]): State of the environment.\n",
    "            action (np.ndarray[(1,), np.float32]): Action applied to the environment.\n",
    "            reward (np.ndarray[(1,), np.float32]): Reward obtained after applying the action.\n",
    "            next_state (np.ndarray[(3,), np.float32]): State of the environment after applying the action.\n",
    "            trunc (np.ndarray[(1,), np.bool]): Boolean indicating if the episode is truncated.\n",
    "        \n",
    "        The buffer is implemented as 5 cyclic (of size `capacity`) numpy array, \n",
    "        one for each element of a transition. \n",
    "        When the buffer is full, the oldest transitions are dropped.\n",
    "        \n",
    "        \n",
    "        Args:\n",
    "            capacity (int): Capacity of the buffer.\n",
    "            transition_type (np.dtype): Type of the transitions to store.\n",
    "        \"\"\"\n",
    "        # Maximum number of transitions to store in the buffer. \n",
    "        # When the buffer overflows the old memories are dropped.\n",
    "        self.capacity = capacity \n",
    "        # Numpy array of transitions\n",
    "        self.states = np.empty((capacity, 3), np.float32)\n",
    "        self.actions = np.empty((capacity, 1), np.float32)\n",
    "        self.rewards = np.empty((capacity, 1), np.float32)\n",
    "        self.next_states = np.empty((capacity, 3), np.float32)\n",
    "        self.truncs = np.empty((capacity, 1), bool)\n",
    "        # Index of the next transition to be stored\n",
    "        self.index = 0\n",
    "        # Current size of the buffer\n",
    "        self.size = 0\n",
    "\n",
    "    def append(self, transition):\n",
    "        \"\"\"Add a transition to the buffer.\n",
    "        \n",
    "        See constructor docstring for explanation of a transition.\n",
    "        \"\"\"\n",
    "        # Add transition to current index\n",
    "        self.states[self.index] = transition[0]\n",
    "        self.actions[self.index] = transition[1]\n",
    "        self.rewards[self.index] = transition[2]\n",
    "        self.next_states[self.index] = transition[3]\n",
    "        self.truncs[self.index] = transition[4]\n",
    "        # Update index\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of size `batch_size` of transitions from the buffers.\"\"\"\n",
    "        indexes = np.random.choice(self.size, batch_size, replace=False)\n",
    "        states = self.states[indexes]\n",
    "        actions = self.actions[indexes]\n",
    "        rewards = self.rewards[indexes]\n",
    "        next_states = self.next_states[indexes]\n",
    "        truncs = self.truncs[indexes]\n",
    "        return (states, actions, rewards, next_states, truncs)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "    \n",
    "# Test buffer\n",
    "buffer = ReplayBuffer(10)\n",
    "for i in range(10):\n",
    "    buffer.append(transition)\n",
    "\n",
    "print(buffer.sample(1)[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"Replay buffer used to store a sequence of transitions.\n",
    "\n",
    "        A transition is a numpy structured array with the fields detailed in the transition_type variable.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def store(self, transition: tuple):\n",
    "        \"\"\"Store a transition in the buffer.\"\"\"\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size: int=128) -> list:\n",
    "        \"\"\"Sample a batch of transitions from the buffer.\"\"\"\n",
    "        indexes = np.random.randint(len(self.buffer), size=batch_size)\n",
    "        \"\"\"return np.asarray(self.buffer)[indexes]\"\"\"\n",
    "        \"\"\"return np.random.choice(np.asarray(self.buffer), batch_size, replace=False)\"\"\"\n",
    "\n",
    "        \"\"\"return np.random.choice(self.buffer, batch_size, replace=False)\"\"\"\n",
    "\n",
    "        batch = []\n",
    "        \"\"\"indexes = np.sort(np.random.choice(len(self.buffer), \n",
    "                                           size=min(batch_size, len(self.buffer)), \n",
    "                                           replace=False))[::-1]\"\"\"\n",
    "        for index in indexes:\n",
    "            batch.append(self.buffer[index])\n",
    "            #self.buffer.pop(index)\n",
    "        return batch\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int=3, action_size: int=1):\n",
    "        \"\"\"QNetwork. Maps (state, action) pairs to Q-values.\"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): State tensor\n",
    "            action (torch.Tensor): Action tensor\n",
    "\n",
    "        Returns:\n",
    "            q_value (torch.Tensor): Q-value tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def TD_learning_update(q_network, agent, optimizer, transitions, gamma):\n",
    "    \"\"\"Perform a 1-step TD-learning update for a batch of transitions.\n",
    "\n",
    "    Args:\n",
    "        q_network (QNetwork): QNetwork instance\n",
    "        agent (Agent): Agent (i.e. Policy) instance\n",
    "        optimizer (torch.optim): Optimizer instance\n",
    "        transitions (List[Transition]): Batch of transitions\n",
    "        gamma (float): Discount factor\n",
    "\n",
    "    Returns:\n",
    "        loss (float): Loss value\n",
    "    \"\"\"\n",
    "    states = transitions[0]\n",
    "    actions = transitions[1]\n",
    "    rewards = transitions[2]\n",
    "    next_states = transitions[3]\n",
    "    truncs = transitions[4]\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    # Note: We choose the convention that all the tensors are of shape \n",
    "    # (batch_size, *), where * is the shape of the data and of dtype\n",
    "    # torch.float32, except for truncs which is of dtype torch.bool\n",
    "    states_t = torch.from_numpy(states)\n",
    "    actions_t = torch.from_numpy(actions)\n",
    "    rewards_t = torch.from_numpy(rewards)\n",
    "    next_states_t = torch.from_numpy(next_states)\n",
    "    truncs_t = torch.from_numpy(truncs)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_actions = agent.compute_action(next_states)\n",
    "        next_actions_t = torch.as_tensor(next_actions)[:, None].float()\n",
    "        # Naive next Q-values (i.e. without taking truncation into account)\n",
    "        naive_next_qs = q_network(next_states_t, next_actions_t)\n",
    "        # Actual next Q-values (i.e. taking truncation into account)\n",
    "        next_qs = torch.where(truncs_t, \n",
    "                              torch.zeros_like(rewards_t), \n",
    "                              naive_next_qs)\n",
    "        gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "        targets = rewards_t + gamma * next_qs\n",
    "\n",
    "    loss = F.mse_loss(q_network(states_t, actions_t), targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity = 10000\n",
    "replay_buffer = ReplayBuffer(capacity=capacity)\n",
    "q_network = QNetwork(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0])\n",
    "optimizer = torch.optim.SGD(q_network.parameters(), lr=1e-4)\n",
    "gamma = 0.99\n",
    "\n",
    "\"\"\"\n",
    "Check dimensionality before and after buffer\n",
    "Check also just at the beginning of the forward method, at the error computation\n",
    "\"\"\"\n",
    "\n",
    "# Training loop for the Q-learning\n",
    "losses = [0] * (1000 * 200 - 128)\n",
    "for episode in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Simulation step\n",
    "        action = heuristic_agent.compute_action(state)\n",
    "        next_state, reward, _, trunc, _ = env.step(action)\n",
    "        replay_buffer.append((state, action, reward, next_state, trunc))\n",
    "        # Training step\n",
    "        if len(replay_buffer) >= 128:\n",
    "            transitions = replay_buffer.sample(128)\n",
    "            loss = TD_learning_update(q_network, heuristic_agent, optimizer, transitions, gamma)\n",
    "            # losses.append(loss.item())\n",
    "            losses[i * episode] = loss.item()\n",
    "\n",
    "        if trunc:\n",
    "            break\n",
    "        state = next_state\n",
    "        i += 1\n",
    "    if episode % 10 == 0 and len(losses) > 0:\n",
    "        print(f'Episode {episode}')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot mean losses over windows of size 2000\n",
    "#plt.plot([np.mean(losses[i:i+2000]) for i in range(len(losses))])\n",
    "# Plot mean losses over batch of 2000\n",
    "size = 2000\n",
    "plt.plot([np.mean(losses[i:i+size]) for i in range(0, len(losses)-size, size)])\n",
    "\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_losses = losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
