{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project: Control in a Continuous Action Space with DDPG\n",
    "\n",
    "In this project, we will explore the Deep Deterministic Policy Gradient (DDPG) algorithm, which is designed to handle continuous action spaces in Reinforcement Learning. We will use the [Pendulum-v1](https://www.gymlibrary.dev/environments/classic_control/pendulum/) environment implemented in OpenAI Gym to implement the DDPG algorithm from scratch to solve the classical control problem of stabilizing an inverted pendulum. Throughout the development, we will incrementally build the components of DDPG and analyze their importance for correct and effective learning. \n",
    "\n",
    "This Jupyter notebook contains our implementation and report for this project. Do not forget to remove the report parts before submitting the notebook.\n",
    "\n",
    "The instructions are available in `Miniproject_DDPG.pdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import gym\n",
    "from helpers import NormalizedEnv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Union\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device: 'cpu' or 'cuda'\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create environement instance, an instance of the Pendulum-v1 environment wrapped in a NormalizedEnv class\n",
    "# to normalize the action space between -1 and 1\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "env = NormalizedEnv(env)\n",
    "\n",
    "# Number of steps per episode, see documentation of the Pendulum-v1 environment\n",
    "n_steps = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random(seed: int = 42) -> None:\n",
    "    \"\"\"Reset numpy, pyTorch and env random seeds.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    env.reset(seed=seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int=env.observation_space.shape[0], action_size: int=env.action_space.shape[0]):\n",
    "        \"\"\"QNetwork. Maps (state, action) pairs to Q-values.\"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): State tensor\n",
    "            action (torch.Tensor): Action tensor\n",
    "\n",
    "        Returns:\n",
    "            q_value (torch.Tensor): Q-value tensor\n",
    "        \"\"\"\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int=3, action_size: int=1):\n",
    "        \"\"\"PolicyNetwork. Maps states to actions.\"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): State tensor\n",
    "\n",
    "        Returns:\n",
    "            action (torch.Tensor): Action tensor\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNoise:\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_noisy_action(self, action: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class GaussianActionNoise(ActionNoise):\n",
    "    def __init__(self, sigma: float=0.3):\n",
    "        \"\"\"Gaussian action noise.\"\"\"\n",
    "        assert(sigma>0), 'sigma must be positive'\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def get_noisy_action(self, action: torch.Tensor) -> torch.Tensor:\n",
    "        noise = torch.normal(mean=0, std=self.sigma, size=action.shape)\n",
    "        return torch.clamp(action + noise, -1, +1)\n",
    "    \n",
    "class OUActionNoise(ActionNoise):\n",
    "    def __init__(self, theta: float=0.15, sigma: float=0.3):\n",
    "        \"\"\"Ornstein-Uhlenbeck action noise.\"\"\"\n",
    "        assert(theta>=0 and theta<=1), 'theta must be between 0 and 1 (included)'\n",
    "        assert(sigma>=0), 'sigma must be non-negative'\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.noise = torch.zeros(env.action_space.shape[0])\n",
    "        \n",
    "    def get_noisy_action(self, action: torch.Tensor) -> torch.Tensor:\n",
    "        self.noise = (1-self.theta)*self.noise + torch.normal(0, self.sigma, size=self.noise.shape)\n",
    "        return torch.clamp(action + self.noise, -1, +1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO docstring\n",
    "class Agent:\n",
    "    def __init__(self, env: NormalizedEnv=env) -> None:\n",
    "        \"\"\"Abstract agent class.\"\"\"\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "    def compute_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, env: NormalizedEnv=env) -> None:\n",
    "        \"\"\"Random agent.\n",
    "        \n",
    "        The random agent uniformly samples actions from the action space.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def compute_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Choose an uniformly random action from the action space.\"\"\"\n",
    "        size = state.shape[:-1] + (self.action_size,)\n",
    "        return np.random.uniform(-1, 1, size=size)\n",
    "    \n",
    "class HeuristicPendulumAgent(Agent):\n",
    "    def __init__(self, torque_intensity: float=1.0, env: NormalizedEnv=env) -> None:\n",
    "        \"\"\"Heuristic pendulum agent.\n",
    "        \n",
    "        The heuristic pendulum agent accelerates the pendulum in the lower half of the domain and decelerates it in the upper half of the domain.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.torque_intensity = torque_intensity\n",
    "        \n",
    "    def compute_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the action to apply to the environment.\n",
    "\n",
    "        When the pendulum is in the lower half of the domain, applies a fixed torque in the same direction as the pendulum's angular velocity.\n",
    "        When the pendulum is in the upper half of the domain, applies a fixed torque in the opposite direction as the pendulum's angular velocity.\n",
    "        \"\"\"\n",
    "        action = np.where(state[..., [0]] < 0, # If the pendulum is in the lower half of the circle\n",
    "                        np.sign(state[..., [2]]) * self.torque_intensity, \n",
    "                        -np.sign(state[..., [2]]) * self.torque_intensity)\n",
    "        return action\n",
    "    \n",
    "class DDPGAgent(Agent):\n",
    "    def __init__(self, action_noise: ActionNoise=GaussianActionNoise(), env: NormalizedEnv=env) -> None:\n",
    "        \"\"\"DDPG agent.\n",
    "        \n",
    "        The DDPG agent uses a policy network to compute actions.\n",
    "        Actions can be noisy or not.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.policy_network = PolicyNetwork(self.state_size, self.action_size).to(device)\n",
    "        self.noise = action_noise\n",
    "        \n",
    "    def compute_action(self, state: np.ndarray, deterministic: bool=True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the action to apply to the environment.\n",
    "\n",
    "        !!! Return a tensor, not a numpy array\n",
    "        \"\"\"\n",
    "        state_t = torch.from_numpy(state).to(device)\n",
    "        action = self.policy_network(state_t)\n",
    "        if not deterministic:\n",
    "            action = self.noise.get_noisy_action(action)\n",
    "        return action\n",
    "    \n",
    "class DDPGAgentWithTargetNetwork(DDPGAgent):\n",
    "    def __init__(self, action_noise: ActionNoise=GaussianActionNoise(), env: NormalizedEnv=env) -> None:\n",
    "        \"\"\"DDPG agent with target network.\n",
    "        \n",
    "        The DDPG agent uses a policy network to compute actions.\n",
    "        The actions can be generated by the policy network or by the target policy network.\n",
    "        The actions can be noisy or not.\n",
    "        \"\"\"\n",
    "        super().__init__(action_noise, env)\n",
    "        # Initialize target network with the same weights as the policy network\n",
    "        self.target_policy_network = PolicyNetwork(self.state_size, self.action_size).to(device)\n",
    "        self.target_policy_network.load_state_dict(self.target_policy_network.state_dict())\n",
    "\n",
    "    def compute_action(self, state: np.ndarray, target: bool=False, deterministic: bool=True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the action to apply to the environment.\n",
    "\n",
    "        !!! Return a tensor, not a numpy array\n",
    "        \"\"\"\n",
    "        state_t = torch.from_numpy(state).to(device)\n",
    "        if target:\n",
    "            action = self.target_policy_network(state_t)\n",
    "        else:\n",
    "            action = self.policy_network(state_t)\n",
    "        if not deterministic:\n",
    "            action = self.noise.get_noisy_action(action)\n",
    "        return action\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"Buffer to store transitions.\n",
    "\n",
    "        A transition is a tuple (state, action, reward, next_state, trunc) where:\n",
    "            state (np.ndarray[(3,), np.float32]): State of the environment.\n",
    "            action (np.ndarray[(1,), np.float32]): Action applied to the environment.\n",
    "            reward (np.ndarray[(1,), np.float32]): Reward obtained after applying the action.\n",
    "            next_state (np.ndarray[(3,), np.float32]): State of the environment after applying the action.\n",
    "            trunc (np.ndarray[(1,), np.bool]): Boolean indicating if the episode is truncated.\n",
    "        \n",
    "        The buffer is implemented as 5 (one for each element of a transition) cyclic numpy arrays of shape \n",
    "        (capacity, *) where * is the shape of the corresponding element of a transition.\n",
    "        When the buffer is full, the oldest transitions are dropped.\n",
    "        \n",
    "        Args:\n",
    "            capacity (int): Capacity of the buffer.\n",
    "        \"\"\"\n",
    "        self.capacity = capacity \n",
    "        self.states = np.empty((capacity, 3), np.float32)\n",
    "        self.actions = np.empty((capacity, 1), np.float32)\n",
    "        self.rewards = np.empty((capacity, 1), np.float32)\n",
    "        self.next_states = np.empty((capacity, 3), np.float32)\n",
    "        self.truncs = np.empty((capacity, 1), bool)\n",
    "        # Index of the next transition to be stored\n",
    "        self.index = 0\n",
    "        # Current size of the buffer\n",
    "        self.size = 0\n",
    "\n",
    "    def append(self, transition):\n",
    "        \"\"\"Add a transition to the buffer.\n",
    "        \n",
    "        See constructor docstring for explanation of a transition.\n",
    "        \"\"\"\n",
    "        # Add transition to current index\n",
    "        self.states[self.index] = transition[0]\n",
    "        self.actions[self.index] = transition[1]\n",
    "        self.rewards[self.index] = transition[2]\n",
    "        self.next_states[self.index] = transition[3]\n",
    "        self.truncs[self.index] = transition[4]\n",
    "        # Update index\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of size `batch_size` of transitions from the buffers.\"\"\"\n",
    "        indexes = np.random.choice(self.size, batch_size, replace=False)\n",
    "        return self[indexes]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, slice):\n",
    "        return (self.states[slice], \n",
    "                self.actions[slice], \n",
    "                self.rewards[slice], \n",
    "                self.next_states[slice], \n",
    "                self.truncs[slice])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_learning_update(q_network: QNetwork, \n",
    "                       agent: Agent, \n",
    "                       optimizer: torch.optim.Optimizer, \n",
    "                       transitions: tuple, \n",
    "                       gamma: float,\n",
    "                       target_q_network: QNetwork=None\n",
    "                       ) -> float:\n",
    "    \"\"\"Perform a 1-step TD-learning update for a batch of transitions.\"\"\"\n",
    "    if isinstance(agent, DDPGAgentWithTargetNetwork) and target_q_network is None:\n",
    "        raise ValueError(\"target_q_network must be provided when agent is a DDPGAgentWithTargetNetwork agent\")\n",
    "    \n",
    "    states = transitions[0]\n",
    "    actions = transitions[1]\n",
    "    rewards = transitions[2]\n",
    "    next_states = transitions[3]\n",
    "    truncs = transitions[4]\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    states_t = torch.from_numpy(states).to(device)\n",
    "    actions_t = torch.from_numpy(actions).to(device)\n",
    "    rewards_t = torch.from_numpy(rewards).to(device)\n",
    "    next_states_t = torch.from_numpy(next_states).to(device)\n",
    "    truncs_t = torch.from_numpy(truncs).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if isinstance(agent, DDPGAgent):\n",
    "            deterministic = True\n",
    "            if isinstance(agent, DDPGAgentWithTargetNetwork): # We use target network for target Q-values\n",
    "                next_actions_t = agent.compute_action(next_states, target=True, deterministic=deterministic)\n",
    "            else:\n",
    "                next_actions_t = agent.compute_action(next_states, deterministic=deterministic)\n",
    "        else:\n",
    "            next_actions = agent.compute_action(next_states)\n",
    "            next_actions_t = torch.as_tensor(next_actions).to(device, dtype=torch.float32)\n",
    "        # Naive next Q-values (i.e. without taking truncation into account)\n",
    "        if isinstance(agent, DDPGAgentWithTargetNetwork): # We use target network for target Q-values\n",
    "            naive_next_qs = target_q_network(next_states_t, next_actions_t)\n",
    "        else:\n",
    "            naive_next_qs = q_network(next_states_t, next_actions_t)\n",
    "        # Actual next Q-values (i.e. taking truncation into account)\n",
    "        next_qs = torch.where(truncs_t, \n",
    "                              torch.zeros_like(rewards_t), \n",
    "                              naive_next_qs)\n",
    "        gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "        targets = rewards_t + gamma * next_qs\n",
    "\n",
    "    loss = F.mse_loss(q_network(states_t, actions_t), targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def policy_learning_update(q_network: QNetwork, \n",
    "                           agent: DDPGAgent, \n",
    "                           optimizer: torch.optim.Optimizer, \n",
    "                           transitions: tuple) -> float:\n",
    "    \"\"\"Perform a policy learning update for a batch of transitions.\"\"\"\n",
    "    states = transitions[0]\n",
    "    # States and actions torch tensors\n",
    "    states_t = torch.from_numpy(states).to(device)\n",
    "    if isinstance(agent, DDPGAgentWithTargetNetwork):\n",
    "        actions_t = agent.compute_action(states, target=False, deterministic=True)\n",
    "    else:\n",
    "        actions_t = agent.compute_action(states, deterministic=True)\n",
    "\n",
    "    # Q-values\n",
    "    qs = q_network(states_t, actions_t)\n",
    "\n",
    "    loss = -qs.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# network is a QNetwork or a PolicyNetwork\n",
    "def target_network_update(network: QNetwork | PolicyNetwork, \n",
    "                          target_network: QNetwork | PolicyNetwork, \n",
    "                          tau: float) -> None:\n",
    "    \"\"\"Update the target network using the following rule:\n",
    "    \n",
    "    theta_target = tau * theta + (1 - tau) * theta_target\n",
    "    \"\"\"\n",
    "    if type(network) != type(target_network):\n",
    "        raise ValueError(\"network and target_network must be of the same type\")\n",
    "    for target_param, param in zip(target_network.parameters(), network.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "def train(agent: Agent,\n",
    "          q_network: QNetwork, \n",
    "          q_optimizer: torch.optim.Optimizer,\n",
    "          policy_optimizer: torch.optim.Optimizer=None,\n",
    "          target_q_network: QNetwork=None,\n",
    "          tau: float=0.01,\n",
    "          n_episodes: int=1000, \n",
    "          batch_size: int=128, \n",
    "          gamma: float=0.99, \n",
    "          buffer_capacity: int=10000, \n",
    "          env: NormalizedEnv=env,\n",
    "          n_steps: int=n_steps, \n",
    "          verbose: bool=True) -> np.ndarray:\n",
    "    \"\"\"Train all the neural networks.\n",
    "\n",
    "    If `agent` is a DDPGAgent, then `policy_optimizer` must be provided and will be used to train the policy.\n",
    "    Otherwise, `policy_optimizer` is ignored and only the Q-network is trained.\n",
    "\n",
    "    Args:\n",
    "        agent: Agent to train\n",
    "        q_network: Q-network\n",
    "        q_optimizer: Optimizer for the Q-network\n",
    "        policy_optimizer: Optimizer for the policy. Used only if `agent` is a DDPGAgent. \n",
    "        target_q_network: Target Q-network. Used only if `agent` is a DDPGAgentWithTargetNetwork.\n",
    "        tau: Tau parameter for the target networks update. Used only if `agent` is a DDPGAgentWithTargetNetwork.\n",
    "        n_episodes: Number of episodes to train for. \n",
    "        batch_size: Batch size for training. \n",
    "        gamma: Discount factor. \n",
    "        buffer_capacity: Capacity of the replay buffer. \n",
    "        env: Environment. \n",
    "        n_steps: Number of steps per episode. \n",
    "        verbose: Whether to print training logs. \n",
    "    \n",
    "    Returns:\n",
    "        If `agent` is not a DDPGAgent, returns the losses for the Q-network and the return of each episode.\n",
    "        If `agent` is a DDPGAgent, returns the losses for the Q-network and the policy and the return of each episode.\n",
    "    \"\"\"\n",
    "    if isinstance(agent, DDPGAgent) and policy_optimizer is None:\n",
    "        raise ValueError(\"Policy optimizer must be provided for DDPGAgent agent.\")\n",
    "    if isinstance(agent, DDPGAgentWithTargetNetwork) and target_q_network is None:\n",
    "        raise ValueError(\"Target Q-network must be provided for DDPGAgenWithTargetNetwork agent.\")\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(capacity=buffer_capacity) # Buffer of transitions\n",
    "    q_losses = np.empty((n_episodes, n_steps)) # Losses for Q-network\n",
    "    if isinstance(agent, DDPGAgent):\n",
    "        policy_losses = np.empty((n_episodes, n_steps)) # Losses for policy network\n",
    "    returns = np.empty(n_episodes) # Return (cumulative reward) for each episode\n",
    "\n",
    "    progress_bar = tqdm(range(n_episodes), desc=\"Training\", disable=not verbose, unit=\"episode\")\n",
    "    with progress_bar as episodes:\n",
    "        for episode in episodes:\n",
    "            state, _ = env.reset()\n",
    "            step = 0\n",
    "            cumulative_reward = 0\n",
    "            while True:\n",
    "                # Simulation step\n",
    "                if isinstance(agent, DDPGAgent):\n",
    "                    # Need this condition to set deterministic parameter.\n",
    "                    # Not deterministic because we want to explore\n",
    "                    deterministic = False\n",
    "                    if isinstance(agent, DDPGAgentWithTargetNetwork):\n",
    "                        # We explicitely set target to False because target network is used only when we update Q-network\n",
    "                        action = agent.compute_action(state, target=False, deterministic=deterministic).detach().cpu().numpy()\n",
    "                    else:\n",
    "                        action = agent.compute_action(state, deterministic=deterministic).detach().cpu().numpy()\n",
    "                else:\n",
    "                    action = agent.compute_action(state)\n",
    "                next_state, reward, _, trunc, _ = env.step(action)\n",
    "                cumulative_reward += reward\n",
    "                transition = (state, action, reward, next_state, trunc)\n",
    "                replay_buffer.append(transition)\n",
    "                # Training step\n",
    "                if len(replay_buffer) >= batch_size:\n",
    "                    transitions = replay_buffer.sample(batch_size)\n",
    "                    # Train the Q-network\n",
    "                    if isinstance(agent, DDPGAgentWithTargetNetwork):\n",
    "                        # We use target network to compute the target Q-values\n",
    "                        q_loss = TD_learning_update(q_network, agent, q_optimizer, transitions, gamma, target_q_network)\n",
    "                    else:\n",
    "                        q_loss = TD_learning_update(q_network, agent, q_optimizer, transitions, gamma)\n",
    "                    # If we have a nan q-loss, we stop the training and raise an error\n",
    "                    if np.isnan(q_loss):\n",
    "                        raise ValueError(\"Loss Q is NaN.\")\n",
    "                    q_losses[episode, step] = q_loss\n",
    "                    # Train the policy\n",
    "                    if isinstance(agent, DDPGAgent):\n",
    "                        # Needed only when the agent is a network\n",
    "                        policy_loss = policy_learning_update(q_network, agent, policy_optimizer, transitions)\n",
    "                        policy_losses[episode, step] = policy_loss\n",
    "                    # Update the target networks\n",
    "                    if isinstance(agent, DDPGAgentWithTargetNetwork):\n",
    "                        # Needed only when we use target network\n",
    "                        target_network_update(q_network, target_q_network, tau=tau)\n",
    "                        target_network_update(agent.policy_network, agent.target_policy_network, tau=tau)\n",
    "\n",
    "                else:\n",
    "                    q_losses[episode, step] = np.nan\n",
    "                    if isinstance(agent, DDPGAgent):\n",
    "                        # Needed only when the agent is a network\n",
    "                        policy_losses[episode, step] = np.nan\n",
    "\n",
    "                if trunc:\n",
    "                    break\n",
    "                state = next_state\n",
    "                step += 1   \n",
    "            returns[episode] = cumulative_reward\n",
    "            # Update the progress bar\n",
    "            episodes.set_postfix_str(\n",
    "                f\"Q-loss: {round(np.nanmean(q_losses[episode]), 2)}\"\n",
    "                + (f\", Policy-loss: {round(np.nanmean(policy_losses[episode]), 2)}\" \n",
    "                   if isinstance(agent, DDPGAgent) else \"\"))\n",
    "    if isinstance(agent, DDPGAgent):\n",
    "        return q_losses, policy_losses, returns\n",
    "    else:\n",
    "        return q_losses, returns\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_return_mean_std(agent: Agent, n_episodes: int = 10, env: NormalizedEnv = env) -> float:\n",
    "    \"\"\"Run the agent for `num_episodes` episodes and return the average return and its standard deviation.\"\"\"\n",
    "    returns = np.empty(n_episodes)\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        cumulative_reward = 0\n",
    "        while True:\n",
    "            # Simulation step\n",
    "            if isinstance(agent, DDPGAgent):\n",
    "                action = agent.compute_action(state, deterministic=True).detach().cpu().numpy()\n",
    "            else:\n",
    "                action = agent.compute_action(state)\n",
    "            state, reward, _, trunc, _ = env.step(action)\n",
    "            cumulative_reward += reward\n",
    "            if trunc:\n",
    "                break\n",
    "        returns[episode] = cumulative_reward\n",
    "    return np.mean(returns), np.std(returns)\n",
    "\n",
    "def save_animation(filename: str, agent: Agent, env: NormalizedEnv = env) -> None:\n",
    "    \"\"\"Run the agent for one episode and save the frames as a gif in `Animations` folder.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    frames = []\n",
    "    while True:\n",
    "        if not isinstance(agent, DDPGAgent):\n",
    "            action = agent.compute_action(state)\n",
    "        else:\n",
    "            action = agent.compute_action(state, deterministic=True).detach().cpu().numpy()\n",
    "        state, _, _, trunc, _ = env.step(action)\n",
    "        frames.append(env.render())\n",
    "        if trunc:\n",
    "            break\n",
    "\n",
    "    frames = np.array(frames)\n",
    "    frames = [Image.fromarray(frame) for frame in frames]\n",
    "    frames[0].save('Animations/' + filename + \".gif\", \n",
    "                   save_all=True, \n",
    "                   append_images=frames[1:], \n",
    "                   duration=50, \n",
    "                   loop=0)\n",
    "\n",
    "def heatmaps(q_networks: list[QNetwork], \n",
    "            q_network_labels: list[str],\n",
    "            velocity_torque_pairs: list[tuple[float, float]],\n",
    "            n_grid: int=100,\n",
    "            show: bool=True, \n",
    "            save: bool=False,\n",
    "            filename: str=None) -> tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"Plot the heatmaps of for each Q-network in `q_networks` for each velocity-torque pair in `velocity_torque_pairs`.\"\"\"\n",
    "    if save and filename is None:\n",
    "        raise ValueError(\"You must specify a name for the file.\")\n",
    "    rad = np.linspace(0, 1, n_grid)\n",
    "    azm = np.linspace(0, 2*np.pi, n_grid)\n",
    "    r, th = np.meshgrid(rad, azm)\n",
    "    q_values = np.empty([len(q_networks), len(velocity_torque_pairs), n_grid, n_grid])\n",
    "    for i, q_network in enumerate(q_networks):\n",
    "        for j, (velocity, torque) in enumerate(velocity_torque_pairs):\n",
    "            states = np.empty([n_grid, n_grid, 3])\n",
    "            states[..., 0] = np.cos(th)\n",
    "            states[..., 1] = np.sin(th)\n",
    "            states[..., 2] = np.full((n_grid, n_grid), velocity)\n",
    "            actions = np.full((n_grid, n_grid, 1), torque)\n",
    "            states_t = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "            actions_t = torch.tensor(actions, dtype=torch.float32).to(device)\n",
    "            temp = q_network(states_t, actions_t).cpu().detach().numpy().squeeze()\n",
    "            q_values[i, j] = temp\n",
    "    # Get min and max q values\n",
    "    q_min = np.min(q_values)\n",
    "    q_max = np.max(q_values)\n",
    "\n",
    "    # Plot a grid of heatmaps\n",
    "    # Each column is a torque_velocity_pair\n",
    "    # First row is q_network before training and second row is q_network after training\n",
    "    fig, axs = plt.subplots(\n",
    "        len(q_networks), len(velocity_torque_pairs), \n",
    "        subplot_kw=dict(projection='polar', \n",
    "                        aspect=1.0, \n",
    "                        theta_offset=np.pi/2), \n",
    "        gridspec_kw=dict(wspace=0.4, hspace=0.2),\n",
    "        figsize=(20, 10)\n",
    "        )\n",
    "    for i, q_network in enumerate(q_networks):\n",
    "        for j, (velocity, torque) in enumerate(velocity_torque_pairs):\n",
    "            ax = axs[i, j]\n",
    "            #ax.set_title('Torque: {}, Velocity: {}'.format(torque, velocity))\n",
    "            ax.set_rticks([])\n",
    "            ax.grid(False)\n",
    "            p = ax.pcolormesh(th, r, q_values[i, j], cmap='viridis', vmin=q_min, vmax=q_max)\n",
    "            if i == 0:\n",
    "                ax.set_title('Velocity: {}, Torque: {}'.format(velocity, torque),\n",
    "                            pad=15, fontsize=14)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(q_network_labels[i],\n",
    "                            labelpad=30, fontsize=14)\n",
    "\n",
    "    fig.colorbar(p, ax=axs.ravel().tolist()).set_label('Q-value', fontsize=14)\n",
    "\n",
    "    if show:\n",
    "        fig.show()\n",
    "    if save:\n",
    "        fig.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "    return fig, axs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random(42) # Reset random seed for reproducibility\n",
    "random_agent = RandomAgent(env)\n",
    "n_episodes = 100\n",
    "random_agent_mean_return, random_agent_std_return = compute_return_mean_std(random_agent, n_episodes, env)\n",
    "print('Average return over {} episodes: {}±{}'.format(n_episodes, round(random_agent_mean_return), round(random_agent_std_return)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random(42) # Reset random seed for reproducibility\n",
    "# Find best torque intensity\n",
    "torque_intensities = np.linspace(0, 1, 11)\n",
    "heuristic_agents = [HeuristicPendulumAgent(torque_intensity=torque_intensity, env=env) for torque_intensity in torque_intensities]\n",
    "mean_returns = []\n",
    "std_returns = []\n",
    "for i, torque_intensity in enumerate(torque_intensities):\n",
    "    heuristic_agent = heuristic_agents[i]\n",
    "    return_mean, return_std = compute_return_mean_std(heuristic_agent, 100, env)\n",
    "    mean_returns.append(return_mean)\n",
    "    std_returns.append(return_std)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(torque_intensities, mean_returns, label='heuristic agent')\n",
    "ax.errorbar(torque_intensities, mean_returns, yerr=std_returns, \n",
    "            fmt=' ', capsize=5, label='Standard deviation')\n",
    "ax.hlines(y=random_agent_mean_return, xmin=0., xmax=1.0, \n",
    "          linestyle = '--', color='black', label='Random agent')\n",
    "ax.set_xlabel('Torque intensity')\n",
    "ax.set_ylabel('Average return over 100 episodes')\n",
    "ax.legend(loc='center right')\n",
    "fig.show()\n",
    "fig.savefig('Figures/3-Average-return.pdf')\n",
    "\n",
    "# (Best) heuristic agent\n",
    "heuristic_agent = heuristic_agents[np.argmax(mean_returns)]\n",
    "#best_torque_intensity = torque_intensities[np.argmax(mean_returns)]\n",
    "#heuristic_agent_mean_return = np.max(mean_returns)\n",
    "#heuristic_agent_std_return = np.max(std_returns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save gif\n",
    "save_animation(\"3-Heuristic-agent\", heuristic_agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment consists of a pendulum attached to a fixed pivot point. Its position is described by the angle $\\alpha$ between the pendulum and the upward vertical direction, and its (angular) velocity is $\\dot{\\alpha}$. The pendulum is actuated by applying a torque $\\tau$ on its pivot point (the action space is thus continuous). The pendulum is subject to gravity, which is the only external force acting on it.\n",
    "\n",
    "The pendulum starts in a random position, with a random velocity and the aim is to stabilize it in the inverted position, using little torque. More precisely, we want to maximise cummulative reward $-\\left(\\alpha^2 + 0.1\\dot{\\alpha}^2 + 0.001\\tau^2\\right)$ over a time horizon of 200 steps. The pendulum starts in a random position, with a random velocity.\n",
    "\n",
    "First we will compare two policies: a random policy and a heuristic policy. \n",
    "The random policy is a policy that selects actions uniformly at random from the action space, i.e. it applies a tork of random magnitude (within the allowed range)in a random direction.\n",
    "The heuristic policy is defined as follows:\n",
    "- When the pendulum is in the lower half of the domain, applies a fixed torque in the same direction as the pendulum's angular velocity;\n",
    "- When the pendulum is in the upper half of the domain, applies a fixed torque in the opposite direction as the pendulum's angular velocity.\n",
    "\n",
    "The average return over 100 episodes and its standard deviation for the random policy is $-1230\\pm 270$, which sets a baseline for the performance of the policies.\n",
    "\n",
    "On Fig. 1, we see the average return over 100 episodes for the heuristic policy depending on the magnitude of the fixed torque. The performance increases linearly from $-1190\\pm330$ for a null fixed torque to $-590\\pm200$ for a fixed torque of $0.5$, and then increases with a smaller slope up to $-430\\pm110$ for a fixed torque of $1$. The exact performance varies from one run to another, but when the performance increases the standard deviation decreases. We observe that the heuristic policy performs better than the random policy for all nonzero values of the fixed torque.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Q function of the heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random(42) # Reset random seed for reproducibility\n",
    "\n",
    "heuristic_q_network = QNetwork().to(device)\n",
    "heuristic_q_network_before_training = copy.deepcopy(heuristic_q_network)\n",
    "optimizer = torch.optim.SGD(heuristic_q_network.parameters(), lr=1e-4)\n",
    "heuristic_q_losses, heuristic_returns = train(\n",
    "    heuristic_agent, \n",
    "    heuristic_q_network, \n",
    "    optimizer, \n",
    "    n_episodes=1000, \n",
    "    batch_size=128, \n",
    "    gamma=0.99, \n",
    "    buffer_capacity=10000, \n",
    "    env=env, \n",
    "    n_steps=n_steps, \n",
    "    verbose=True)\n",
    "\n",
    "# Save trained network\n",
    "# torch.save(heuristic_q_network.state_dict(), 'Networks/4-Heuristic-Q-network.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# Plot the mean loss per episode\n",
    "ax.plot(np.nanmean(heuristic_q_losses, axis=1), label='Mean loss', color='C0')\n",
    "# Add linear regression line\n",
    "x = np.arange(heuristic_q_losses.shape[0])\n",
    "slope, intercept, _, _, _ = stats.linregress(x, np.nanmean(heuristic_q_losses, axis=1))\n",
    "ax.plot(x, slope*x + intercept, 'k--', label='Linear regression, slope: {:.2f}'.format(slope))\n",
    "\n",
    "# Add legend and labels\n",
    "ax.legend()\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Mean Q-loss', fontsize=12)\n",
    "fig.show()\n",
    "fig.savefig('Figures/4-Training.pdf', bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement a critic, i.e. a Q-network, whose goal is to estimate the Q-function of the heuristic policy. We use a neural network with two hidden layers of 32 neurons each, with ReLU activation functions. It is updated using the semi-gradient of the mean squared error between the predicted and target Q-values. The training is done over 1000 epochs, with a batch size of 64.\n",
    "\n",
    "On Fig. 2 we see the training curve with batches of size 128 and 1000 epochs. We can see that the training is very fast, with the loss decreasing to a value of approximately 70 after only 100 episodes. The loss then decreases more slowly, reaching a value close to 50 after 1000 episodes.\n",
    "\n",
    "\n",
    "\n",
    "[TODO] comment the heatmaps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Minimal implementation of DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random(0) # Reset random seed for reproducibility\n",
    "\n",
    "ddpg_q_network = QNetwork().to(device)\n",
    "action_noise = GaussianActionNoise(sigma=0.3)\n",
    "ddpg_agent = DDPGAgent(action_noise=action_noise)\n",
    "q_optimizer = torch.optim.SGD(ddpg_q_network.parameters(), lr=1e-4)\n",
    "policy_optimizer = torch.optim.SGD(ddpg_agent.policy_network.parameters(), lr=1e-4)\n",
    "\n",
    "ddpg_q_losses, ddpg_policy_losses, ddpg_returns = train(\n",
    "    ddpg_agent, \n",
    "    ddpg_q_network, \n",
    "    q_optimizer, \n",
    "    policy_optimizer, \n",
    "    n_episodes=1000, \n",
    "    batch_size=128, \n",
    "    gamma=0.99, \n",
    "    buffer_capacity=10000)\n",
    "\n",
    "# Save trained network\n",
    "torch.save(ddpg_q_network.state_dict(), 'Networks/5-DDPG-Q-network.pt')\n",
    "torch.save(ddpg_agent.policy_network.state_dict(), 'Networks/5-DDPG-policy-network.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot q and policy training curves and return on a subplot under\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "# Q training curve\n",
    "ax1.plot(np.nanmean(ddpg_q_losses, axis=1), label='Q-network', color='C0')\n",
    "# Policy training curve\n",
    "ax1.plot(np.nanmean(ddpg_policy_losses, axis=1), label='Policy network', color='C2')\n",
    "\n",
    "ax1.set_ylabel('Mean loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Moving average smoothing of the returns\n",
    "ax2.plot(ddpg_returns, alpha=0.5, label='Return', color='C1')\n",
    "window = 50\n",
    "ax2.plot(np.convolve(ddpg_returns, np.ones(window)/window, mode='valid'), \n",
    "        label='Moving average (window = {})'.format(window), color='C1')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Return')\n",
    "ax2.legend()\n",
    "fig.show()\n",
    "fig.savefig('Figures/5-Training.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "reset_random(42)\n",
    "n_episodes = 100\n",
    "ddpg_agent_mean_return, ddpg_agent_standard_deviation = compute_return_mean_std(ddpg_agent, n_episodes, env)\n",
    "print('Average return over {} episodes: {}±{}'.format(n_episodes, round(ddpg_agent_mean_return), round(ddpg_agent_standard_deviation)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_networks = [heuristic_q_network_before_training, heuristic_q_network, ddpg_q_network]\n",
    "q_networks_labels = ['Untrained Q-network', 'Heuristic Q-network', 'DDPG Q-network']\n",
    "velocity_torque_pairs = [(0,0), (1,1), (1,-1), (5,1), (5,-1)]\n",
    "\n",
    "fig, axes = heatmaps(\n",
    "    q_networks,\n",
    "    q_networks_labels,\n",
    "    velocity_torque_pairs,\n",
    "    n_grid=100,\n",
    "    show=True,\n",
    "    save=True,\n",
    "    filename='Figures/4-5-Heatmaps.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save gif\n",
    "save_animation(\"5-DDPG-agent\", ddpg_agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we replace the heuristic agent by an agent equipped with a policy network that compute the next action to take based on the current state. We use a neural network with two hidden layers of 32 neurons each, with ReLU activation functions and a tanh activation on the output layer, forcing the returned action to be between -1 and 1. The policy network is updated in order to maximize the mean Q-value. The training is done over 1000 epochs, with a batch size of 128."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random(42)\n",
    "taus = [0.01, 0.2, 0.5, 0.8, 1.0]#np.logspace(-2, 0, 5)\n",
    "tau_q_networks = []\n",
    "tau_target_q_networks = []\n",
    "tau_ddpg_agents_with_target_network = []\n",
    "tau_q_losses = []\n",
    "tau_policy_losses = []\n",
    "tau_returns = []\n",
    "for i, tau in enumerate(taus):\n",
    "    # Train\n",
    "    q_network = QNetwork().to(device)\n",
    "    target_q_network = copy.deepcopy(q_network)\n",
    "    action_noise = GaussianActionNoise(sigma=0.3)\n",
    "    ddpg_agent_with_target_network = DDPGAgentWithTargetNetwork(action_noise=action_noise)\n",
    "    q_optimizer = torch.optim.SGD(q_network.parameters(), lr=1e-4)\n",
    "    policy_optimizer = torch.optim.SGD(ddpg_agent_with_target_network.policy_network.parameters(), lr=1e-4)\n",
    "\n",
    "    q_losses, policy_losses, returns = train(\n",
    "        ddpg_agent_with_target_network, \n",
    "        q_network, \n",
    "        q_optimizer, \n",
    "        policy_optimizer, \n",
    "        target_q_network, \n",
    "        tau=tau, \n",
    "        n_episodes=1000, \n",
    "        batch_size=128, \n",
    "        gamma=0.99, \n",
    "        buffer_capacity=10000)\n",
    "    \n",
    "    # Store results\n",
    "    tau_q_networks.append(q_network)\n",
    "    tau_target_q_networks.append(target_q_network)\n",
    "    tau_ddpg_agents_with_target_network.append(ddpg_agent_with_target_network)\n",
    "    tau_q_losses.append(q_losses)\n",
    "    tau_policy_losses.append(policy_losses)\n",
    "    tau_returns.append(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot q and policy training curves and return on a subplot under\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(6, 8))\n",
    "\n",
    "for i, tau in enumerate(taus):\n",
    "        # Q training curve\n",
    "        ax1.plot(np.nanmean(tau_q_losses[i], axis=1), label='τ={}'.format(round(tau, 2)))\n",
    "        # Policy training curve\n",
    "        ax2.plot(np.nanmean(tau_policy_losses[i], axis=1), label='τ={}'.format(round(tau, 2)))\n",
    "\n",
    "        # Moving average smoothing of the returns\n",
    "        #ax3.plot(lin_tau_returns[i], alpha=0.4, label='Return, tau={}'.format(round(tau, 2)))\n",
    "        window = 50\n",
    "        ax3.plot(np.convolve(tau_returns[i], np.ones(window)/window, mode='valid'), \n",
    "                label='τ={}'.format(round(tau, 2)))\n",
    "\n",
    "ax1.set_ylabel('Mean loss', fontsize=12)\n",
    "ax1.legend(loc='lower center', bbox_to_anchor=(0.42, 1.1), ncol=5)\n",
    "ax1.set_title('Q-network learning curves')\n",
    "ax2.set_ylabel('Mean loss', fontsize=12)\n",
    "ax2.set_title('Policy network learning curves')\n",
    "ax3.set_xlabel('Episode', fontsize=12)\n",
    "ax3.set_ylabel('Return', fontsize=12)\n",
    "ax3.set_title('Smoothed returns (moving average window = {})'.format(window))\n",
    "fig.show()\n",
    "fig.savefig('Figures/6-Training.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "tau_ddpg_agent_with_target_network_mean_returns = [] \n",
    "tau_ddpg_agent_with_target_network_standard_deviations = []\n",
    "for i, tau in enumerate(taus):\n",
    "    n_episodes = 100\n",
    "    mean_return, standard_deviation = compute_return_mean_std(tau_ddpg_agents_with_target_network[i], n_episodes)\n",
    "    tau_ddpg_agent_with_target_network_mean_returns.append(mean_return)\n",
    "    tau_ddpg_agent_with_target_network_standard_deviations.append(standard_deviation)\n",
    "    print('Average return over {} episodes, tau={}: {}±{}'.format(n_episodes, round(tau, 2), round(mean_return), round(standard_deviation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save gifs\n",
    "for i, tau in enumerate(taus):\n",
    "    save_animation(\"6-DDPG-agent-with-target-network-tau={}\".format(round(tau, 2)), tau_ddpg_agents_with_target_network[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When experimenting with the first implementation of DDPG above, we noticed that depending on the choice of the seed, the training was unstable. This motivates the use of target networks, which are copies of the actor and critic networks that are \"soft updated\". This means that each step they are updated with a weighted average between the weights of the target network and the weights of the original network, i.e. each weight $w_{target}$ of the actor/critic target network are updated using this equation:\n",
    "$$\n",
    "w_\\mathrm{target}^{(t+1)} = \\tau\\cdot w_\\mathrm{original}^{(t)} + (1-\\tau)\\cdot w_\\mathrm{target}^{(t)}\n",
    "$$\n",
    "with $\\tau\\in (0, 1)$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Ornstein-Uhlenbeck noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random(42)\n",
    "thetas = [0, 0.25, 0.5, 0.75, 1]\n",
    "theta_ddpg_agents_with_target_network = []\n",
    "theta_q_losses = []\n",
    "theta_policy_losses = []\n",
    "theta_returns = []\n",
    "for theta in thetas:\n",
    "    q_network = QNetwork().to(device)\n",
    "    target_q_network = copy.deepcopy(q_network)\n",
    "    action_noise = OUActionNoise(theta=theta, sigma=0.3)\n",
    "    ddpg_agent_with_target_network = DDPGAgentWithTargetNetwork(action_noise, env)\n",
    "    q_optimizer = torch.optim.SGD(q_network.parameters(), lr=1e-4)\n",
    "    policy_optimizer = torch.optim.SGD(ddpg_agent_with_target_network.policy_network.parameters(), lr=1e-4)\n",
    "\n",
    "    q_losses, policy_losses, returns = train(\n",
    "        ddpg_agent_with_target_network, \n",
    "        q_network, \n",
    "        q_optimizer, \n",
    "        policy_optimizer, \n",
    "        target_q_network, \n",
    "        tau=0.5,\n",
    "        n_episodes=1000, \n",
    "        batch_size=128, \n",
    "        gamma=0.99, \n",
    "        buffer_capacity=10000)\n",
    "    \n",
    "    theta_ddpg_agents_with_target_network.append(ddpg_agent_with_target_network)\n",
    "    theta_q_losses.append(q_losses)\n",
    "    theta_policy_losses.append(policy_losses)\n",
    "    theta_returns.append(returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot q and policy training curves and return on a subplot under\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(6, 8))\n",
    "\n",
    "for i, theta in enumerate(thetas):\n",
    "        # Q training curve\n",
    "        ax1.plot(np.nanmean(theta_q_losses[i], axis=1), label='θ={}'.format(round(theta, 2)))\n",
    "        # Policy training curve\n",
    "        ax2.plot(np.nanmean(theta_policy_losses[i], axis=1), label='θ={}'.format(round(theta, 2)))\n",
    "\n",
    "        # Moving average smoothing of the returns\n",
    "        #ax3.plot(lin_tau_returns[i], alpha=0.4, label='Return, tau={}'.format(round(tau, 2)))\n",
    "        window = 50\n",
    "        ax3.plot(np.convolve(theta_returns[i], np.ones(window)/window, mode='valid'), \n",
    "                label='θ={}'.format(round(theta, 2)))\n",
    "\n",
    "ax1.set_ylim([0, 2000])\n",
    "ax1.set_ylabel('Mean loss', fontsize=12)\n",
    "ax1.legend(loc='lower center', bbox_to_anchor=(0.42, 1.1), ncol=5)\n",
    "ax1.set_title('Q-network learning curves')\n",
    "ax2.set_ylabel('Mean loss', fontsize=12)\n",
    "ax2.set_title('Policy network learning curves')\n",
    "ax3.set_xlabel('Episode', fontsize=12)\n",
    "ax3.set_ylabel('Return', fontsize=12)\n",
    "ax3.set_title('Smoothed returns (moving average window = {})'.format(window))\n",
    "fig.show()\n",
    "fig.savefig('Figures/7-Training.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_ddpg_agent_with_target_network_mean_returns = [] \n",
    "theta_ddpg_agent_with_target_network_standard_deviations = []\n",
    "for i, theta in enumerate(thetas):\n",
    "    n_episodes = 100\n",
    "    mean_return, standard_deviation = compute_return_mean_std(theta_ddpg_agents_with_target_network[i], n_episodes)\n",
    "    theta_ddpg_agent_with_target_network_mean_returns.append(mean_return)\n",
    "    theta_ddpg_agent_with_target_network_standard_deviations.append(standard_deviation)\n",
    "    print('Average return over {} episodes, tau={}: {}±{}'.format(n_episodes, round(theta, 2), round(mean_return), round(standard_deviation)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding noise to the action helps exploration of the state space, which is necessary to learn a good policy. However, adding uncorrelated noise to subsequent actions is not very effective in practice, as it leads to trajectories close to the deterministic one. To improve the exploration of the algorithm, we replace the Gaussian noise with a (simplified) Ornstein-Uhlenbeck noise defined by the iterative equation:\n",
    "$$\n",
    "\\mathrm{noise}^{(t+1)} = (1-\\theta)\\cdot \\mathrm{noise}^{(t)} + \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "with $\\theta\\in (0, 1)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
