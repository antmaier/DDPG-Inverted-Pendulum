{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project: Control in a Continuous Action Space with DDPG\n",
    "\n",
    "In this project, we will explore the Deep Deterministic Policy Gradient (DDPG) algorithm, which is designed to handle continuous action spaces in Reinforcement Learning. We will use the Pendulum-v1 environment implemented in OpenAI Gym to implement the DDPG algorithm from scratch to solve the classical control problem of stabilizing an inverted pendulum. Throughout the development, we will incrementally build the components of DDPG and analyze their importance for correct and effective learning. This Jupyter notebook contains our implementation and report for this project. \n",
    "\n",
    "The instructions are available in `Miniproject_DDPG.pdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import gym\n",
    "from helpers import NormalizedEnv\n",
    "from helpers import RandomAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device: 'cpu' or 'cuda'\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environement instance, an instance of the Pendulum-v1 environment wrapped in a NormalizedEnv class\n",
    "# to normalize the action space between -1 and 1\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "env = NormalizedEnv(env)\n",
    "\n",
    "# Create a RandomAgent\n",
    "random_agent = RandomAgent(env)\n",
    "\n",
    "def run_agent(agent: 'Agent', env: NormalizedEnv, num_episodes: int) -> float:\n",
    "    \"\"\"Run the agent for `num_episodes` episodes and return the average reward\"\"\"\n",
    "    rewards = [] # Will contain the reward of each step\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            # Simulation step\n",
    "            action = agent.compute_action(state)\n",
    "            state, reward, _, trunc, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if trunc:\n",
    "                break\n",
    "        rewards.append(episode_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "average_reward = run_agent(random_agent, env, 10)\n",
    "print('Average reward over 10 episodes: {}'.format(average_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeuristicPendulumAgent:\n",
    "    def __init__(self, env: NormalizedEnv, torque_intensity: float=1.0) -> None:\n",
    "        \"\"\"Heuristic agent for the Pendulum-v1 environment.\"\"\"\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.torque_intensity = torque_intensity\n",
    "        \n",
    "    def compute_action(self, state) -> float:\n",
    "        \"\"\"Compute the action to apply to the environment.\n",
    "\n",
    "        When the pendulum is in the lower half of the domain, applies a fixed torque in the same direction as the pendulum's angular velocity.\n",
    "        When the pendulum is in the upper half of the domain, applies a fixed torque in the opposite direction as the pendulum's angular velocity.\n",
    "\n",
    "        Warning: it returns a float, not a numpy array.\n",
    "        \"\"\"\n",
    "        action = np.where(state[..., 0] < 0, # If the pendulum is in the lower half of the circle\n",
    "                        np.sign(state[..., 2]) * self.torque_intensity, \n",
    "                        -np.sign(state[..., 2]) * self.torque_intensity)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best torque intensity\n",
    "torque_intensities = np.linspace(0.1, 1, 10)\n",
    "rewards = []\n",
    "for torque_intensity in torque_intensities:\n",
    "    heuristic_agent = HeuristicPendulumAgent(env, torque_intensity=torque_intensity)\n",
    "    rewards.append(run_agent(heuristic_agent, env, 10))\n",
    "\n",
    "plt.plot(torque_intensities, rewards)\n",
    "plt.xlabel('Torque intensity')\n",
    "plt.ylabel('Average reward over 10 episodes')\n",
    "plt.show()\n",
    "\n",
    "# Create a HeuristicPendulumAgent with the best torque intensity\n",
    "heuristic_agent = HeuristicPendulumAgent(env, torque_intensity=torque_intensities[np.argmax(rewards)])\n",
    "# Print best reward over 10 episodes and associated torque intensity\n",
    "print('Best torque intensity: {}'.format(torque_intensities[np.argmax(rewards)]))\n",
    "print('Best reward over 10 episodes: {}'.format(np.max(rewards)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation of the Heuristic Agent using PIL\n",
    "state, _ = env.reset()\n",
    "frames = []\n",
    "while True:\n",
    "    action = heuristic_agent.compute_action(state)\n",
    "    state, reward, _, trunc, _ = env.step(action)\n",
    "    frames.append(env.render())\n",
    "    if trunc:\n",
    "        break\n",
    "\n",
    "frames = np.array(frames)\n",
    "frames = [Image.fromarray(frame) for frame in frames]\n",
    "frames[0].save(\"heuristic_agent.gif\", save_all=True, append_images=frames[1:], duration=50, loop=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Q function of the heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"Buffer to store transitions.\n",
    "\n",
    "        A transition is a tuple (state, action, reward, next_state, trunc) where:\n",
    "            state (np.ndarray[(3,), np.float32]): State of the environment.\n",
    "            action (np.ndarray[(1,), np.float32]): Action applied to the environment.\n",
    "            reward (np.ndarray[(1,), np.float32]): Reward obtained after applying the action.\n",
    "            next_state (np.ndarray[(3,), np.float32]): State of the environment after applying the action.\n",
    "            trunc (np.ndarray[(1,), np.bool]): Boolean indicating if the episode is truncated.\n",
    "        \n",
    "        The buffer is implemented as 5 (one for each element of a transition) cyclic numpy arrays of shape \n",
    "        (capacity, *) where * is the shape of the corresponding element of a transition.\n",
    "        When the buffer is full, the oldest transitions are dropped.\n",
    "        \n",
    "        Args:\n",
    "            capacity (int): Capacity of the buffer.\n",
    "            transition_type (np.dtype): Type of the transitions to store.\n",
    "        \"\"\"\n",
    "        # Maximum number of transitions to store in the buffer. \n",
    "        # When the buffer overflows the old memories are dropped.\n",
    "        self.capacity = capacity \n",
    "        # Numpy array of transitions\n",
    "        self.states = np.empty((capacity, 3), np.float32)\n",
    "        self.actions = np.empty((capacity, 1), np.float32)\n",
    "        self.rewards = np.empty((capacity, 1), np.float32)\n",
    "        self.next_states = np.empty((capacity, 3), np.float32)\n",
    "        self.truncs = np.empty((capacity, 1), bool)\n",
    "        # Index of the next transition to be stored\n",
    "        self.index = 0\n",
    "        # Current size of the buffer\n",
    "        self.size = 0\n",
    "\n",
    "    def append(self, transition):\n",
    "        \"\"\"Add a transition to the buffer.\n",
    "        \n",
    "        See constructor docstring for explanation of a transition.\n",
    "        \"\"\"\n",
    "        # Add transition to current index\n",
    "        self.states[self.index] = transition[0]\n",
    "        self.actions[self.index] = transition[1]\n",
    "        self.rewards[self.index] = transition[2]\n",
    "        self.next_states[self.index] = transition[3]\n",
    "        self.truncs[self.index] = transition[4]\n",
    "        # Update index\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of size `batch_size` of transitions from the buffers.\"\"\"\n",
    "        indexes = np.random.choice(self.size, batch_size, replace=False)\n",
    "        return self[indexes]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, slice):\n",
    "        return (self.states[slice], \n",
    "                self.actions[slice], \n",
    "                self.rewards[slice], \n",
    "                self.next_states[slice], \n",
    "                self.truncs[slice])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int=3, action_size: int=1):\n",
    "        \"\"\"QNetwork. Maps (state, action) pairs to Q-values.\"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): State tensor\n",
    "            action (torch.Tensor): Action tensor\n",
    "\n",
    "        Returns:\n",
    "            q_value (torch.Tensor): Q-value tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def TD_learning_update(q_network, agent, optimizer, transitions, gamma):\n",
    "    \"\"\"Perform a 1-step TD-learning update for a batch of transitions.\n",
    "\n",
    "    Args:\n",
    "        q_network (QNetwork): QNetwork instance\n",
    "        agent (Agent): Agent (i.e. Policy) instance\n",
    "        optimizer (torch.optim): Optimizer instance\n",
    "        transitions (List[Transition]): Batch of transitions\n",
    "        gamma (float): Discount factor\n",
    "\n",
    "    Returns:\n",
    "        loss (float): Loss value\n",
    "    \"\"\"\n",
    "    states = transitions[0]\n",
    "    actions = transitions[1]\n",
    "    rewards = transitions[2]\n",
    "    next_states = transitions[3]\n",
    "    truncs = transitions[4]\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    states_t = torch.from_numpy(states).to(device)\n",
    "    actions_t = torch.from_numpy(actions).to(device)\n",
    "    rewards_t = torch.from_numpy(rewards).to(device)\n",
    "    next_states_t = torch.from_numpy(next_states).to(device)\n",
    "    truncs_t = torch.from_numpy(truncs).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_actions = agent.compute_action(next_states)\n",
    "        next_actions_t = torch.as_tensor(next_actions)[:, None].to(device, dtype=torch.float32)\n",
    "        # Naive next Q-values (i.e. without taking truncation into account)\n",
    "        naive_next_qs = q_network(next_states_t, next_actions_t)\n",
    "        # Actual next Q-values (i.e. taking truncation into account)\n",
    "        next_qs = torch.where(truncs_t, \n",
    "                              torch.zeros_like(rewards_t), \n",
    "                              naive_next_qs)\n",
    "        gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "        targets = rewards_t + gamma * next_qs\n",
    "\n",
    "    loss = F.mse_loss(q_network(states_t, actions_t), targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity = 10000 # Capacity of the replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity=capacity)\n",
    "q_network = QNetwork(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0]).to(device)\n",
    "optimizer = torch.optim.SGD(q_network.parameters(), lr=1e-4)\n",
    "gamma = 0.99\n",
    "\n",
    "# Training loop for the Q-learning\n",
    "n_episodes = 1000 # Number of episodes\n",
    "# Number of steps per episode, see documentation of the Pendulum-v1 environment\n",
    "n_steps = 200\n",
    "batch_size = 128 # Batch size for the TD-learning update\n",
    "# Container for the loss of each step of each episode\n",
    "losses = np.empty((n_episodes, n_steps))\n",
    "logging_interval = 10\n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    while True:\n",
    "        # Simulation step\n",
    "        action = heuristic_agent.compute_action(state)\n",
    "        next_state, reward, _, trunc, _ = env.step(action)\n",
    "        replay_buffer.append((state, action, reward, next_state, trunc))\n",
    "        # Training step\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            transitions = replay_buffer.sample(batch_size)\n",
    "            loss = TD_learning_update(q_network, heuristic_agent, optimizer, transitions, gamma)\n",
    "            losses[episode, step] = loss.item()\n",
    "        else:\n",
    "            losses[episode, step] = np.nan\n",
    "\n",
    "        if trunc:\n",
    "            break\n",
    "        state = next_state\n",
    "        step += 1\n",
    "    # Logging average loss of the last episode every `logging_interval` episodes\n",
    "    if episode % logging_interval == 0 and episode > 0:\n",
    "        print(f'Episode {episode}/{n_episodes-1}, average loss: {np.mean(losses[episode])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean loss per episode\n",
    "plt.plot(np.nanmean(losses, axis=1))\n",
    "# Add linear regression line\n",
    "x = np.arange(n_episodes)\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, np.nanmean(losses, axis=1))\n",
    "plt.plot(x, slope*x + intercept, 'r--')\n",
    "\n",
    "# legend\n",
    "plt.legend(['Mean loss', 'Linear regression, slope: {:.2f}'.format(slope)])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: demander ce qu'on doit faire exactement pour le heatmap, c'est quoi les 3 axes?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Minimal implementation of DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int=3, action_size: int=1):\n",
    "        \"\"\"PolicyNetwork. Maps states to actions.\"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): State tensor\n",
    "\n",
    "        Returns:\n",
    "            action (torch.Tensor): Action tensor\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
