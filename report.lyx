#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 1.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center} 
\backslash
huge
\backslash
textbf{Miniproject - DDPG}
\backslash

\backslash
%
\end_layout

\begin_layout Plain Layout


\backslash
large
\backslash
sffamily Antoine Maier, Aude Maier
\backslash

\backslash
%
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
In this project, we will explore the Deep Deterministic Policy Gradient
 (DDPG) algorithm, which is designed to handle continuous action spaces
 in Reinforcement Learning.
 We will use the 
\begin_inset CommandInset href
LatexCommand href
name "Pendulum-v1"
target "https://www.gymlibrary.dev/environments/classic_control/pendulum/"
literal "false"

\end_inset

 environment implemented in OpenAI Gym to implement the DDPG algorithm from
 scratch to solve the classical control problem of stabilising an inverted
 pendulum.
 Throughout the development, we will incrementally build the components
 of DDPG and explore the impact of target networks and Ornstein-Uhlenbeck
 noise on the learning process.
\end_layout

\begin_layout Section*
3 Heuristic Policy
\end_layout

\begin_layout Standard
In this work, the action space is renormalised to 
\begin_inset Formula $[-1,1]$
\end_inset

 where -1 corresponds to the maximum torque in the clockwise direction and
 1 to the maximum torque in the counterclockwise direction.
 This convention is used throughout the rest of this report.
\end_layout

\begin_layout Standard
First we will compare two policies: a random policy that selects actions
 uniformly at random from the action space, and a heuristic policy designed
 to accelerate the pendulum when on the lower half of the domain and deccelerate
 it otherwise.
\end_layout

\begin_layout Standard
The average return over 100 episodes and its standard deviation for the
 random policy is 
\begin_inset Formula $-1230\pm270$
\end_inset

, which sets a baseline for the performance of the policies.
\end_layout

\begin_layout Standard
On Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we see the average return over 100 episodes for the heuristic policy depending
 on the magnitude of the fixed torque.
 The performance increases linearly from 
\begin_inset Formula $-1190\pm330$
\end_inset

 for a null fixed torque to 
\begin_inset Formula $-590\pm200$
\end_inset

 for a fixed torque of 
\begin_inset Formula $0.5$
\end_inset

, and then increases with a smaller slope up to 
\begin_inset Formula $-430\pm110$
\end_inset

 for a fixed torque of 
\begin_inset Formula $1$
\end_inset

.
 The exact performance varies from one run to another, but when the performance
 increases the standard deviation decreases.
 We observe that the heuristic policy performs better than the random policy
 for all nonzero values of the fixed torque.
\end_layout

\begin_layout Standard
The performance of these two policies can be compared to the next policies
 of this homework in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:appendix-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Section*
4 Q-function of the heuristic policy
\end_layout

\begin_layout Standard
We now implement a critic, i.e.
 a Q-network, whose goal is to estimate the Q-function of the heuristic
 policy with torque intensity 1 (i.e.
 the value that achieved the highest cumulative reward in the previous section).
 We use a neural network with two hidden layers of 32 neurons each, with
 ReLU activation functions.
 It is updated using the semi-gradient of the mean squared error between
 the predicted and target Q-values.
 
\end_layout

\begin_layout Standard
In Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:4-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we see the training curve with batches of size 128 and 1000 epochs.
 We observe a rapid increase of the Q-loss during the first episodes.
 This is due to the fact that the replay buffer is almost empty at that
 stage and therefore the network is trained on the same (or very similar)
 samples over and over.
 We will encounter this phenomenon for every network training in this work.
 After this initial peak, the loss decreases rapidely to a value of approximatel
y 70 after 100 episodes.
 The training then slows down, with the loss reaching a value close to 50
 after 1000 episodes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang french
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/3-Average-return.pdf
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:3"

\end_inset

Average return of the heuristic agent and the random agent over 100 episodes.
 The heuristic agent is trained with 11 different torque intensities between
 0 and 1.
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/4-Training.pdf
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:4-training"

\end_inset

Mean Q-loss per episode during training of the Q-network for the heuristic
 agent.
 The slope of the linear regression line is -0.01.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/4-5-Heatmaps.png
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:4-5-Heatmaps"

\end_inset

Q-value as a function of the position of the pendulum, for a given velocity
 and torque.
 The two first rows plot the Q-values computed by the heuristic Q-network
 before and after training, respectively.
 The third row shows the Q-values computed by the Q-network of the DDPG
 agent.
 For each row, five different velocity-torque pairs are considered.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We then plot and compare the Q-values given by the Q-network before and
 after training, for several values of torque and velocity, by means of
 heatmaps.
 The result is shown in the top two rows of Fig.Â 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:4-5-Heatmaps"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where a positive velocity corresponds to the pendulum moving in the trigonomet
ric direction and a positive torque corresponds to a torque applied in the
 trigonometric direction.
\end_layout

\begin_layout Standard
The Q-values before training are almost null constants compared to after
 the training.
 In fact, if plotted on a different color scale, we would see that the Q-values
 before training are a mix of randomness and continuity of order 
\begin_inset Formula $O(10^{-2})$
\end_inset

, which is easily explained by the random initialization of the weights
 and the intrinsic continuity of the network, and it is certainly not due
 to the fact that the policy achieves a high return and that the Q-network
 is able to predict it.
\end_layout

\begin_layout Standard
The Q-values after training are 4 orders of magnitude larger (in absolute
 value).
 The heatmap corresponding to velocity=0 and torque=0 shows an accurate
 representation of the reward distribution, i.e.
 the closer you are to 
\begin_inset Formula $\alpha=0^{\circ}$
\end_inset

, the more reward you get and the easier it will be to be close to 
\begin_inset Formula $\alpha=0^{\circ}$
\end_inset

 in the future.
 A notable fact is that this representation appeared without any explicit
 training of the action corresponding to torque=0 but is an interpolation
 of the knowledge acquired for torque=
\begin_inset Formula $\pm1$
\end_inset

.
\end_layout

\begin_layout Standard
The heatmap of velocity=1, torque=-1 is similar to the previous one, but
 with a slight rotation in the clockwise direction.
 This is due to the fact that with this (small) velocity, the heuristic
 policy will decelerate the pendulum to try to balance it in the inverted
 position, thus leading to a high reward for a small negative angle.
 We note that the lower left and right quadrants are situations that the
 Q-network has never experienced, since the heuristic policy is perfectly
 deterministic.
 With velocity=1 but torque=1, these are the two upper quadrants that are
 unexperienced situations for the Q-network.
 However, the heatmap is very similar to the previous one, hence it looks
 as if the Q-network has inferred the Q-values for the unexperienced situations
 from the experienced ones.
 Unfortunately, we can doubt that in such a situation of positive velocity
 and accelerated pendulum the Q-values would be as high as claimed by the
 Q-network.
\end_layout

\begin_layout Standard
Then, the heatmap with velocity=5 and torque=-1 resembles the one with velocity=
1 and torque=-1, but with the high-reward circular sector rotated in the
 clockwise direction.
 This is due to the fact that with a greater velocity, the heuristic policy
 will be able to balance the pendulum if it decelerates it earlier.
 This time, we chose to change sign of both the velocity and the torque.
 We expect the heatmap to be the mirrored image of the previous one, which
 holds up to a certain extent.
 We can imagine that the differrences between the two heatmaps are due to
 the fact that the Q-network has not been perfectly trained.
\end_layout

\begin_layout Section*
5 Minimal implementation of DDPG
\end_layout

\begin_layout Standard
In this section, we replace the heuristic agent with an agent equipped with
 a policy network that compute the next action to take based on the current
 state.
 We use a neural network with two hidden layers of 32 neurons each, with
 ReLU activation functions, and a tanh activation on the output layer, ensuring
 that the returned action is between -1 and 1.
 The policy network is updated to maximize the mean Q-value.
 To enable better exploration of the state space when generating new transitions
, a small Gaussian noise is added to the action returned by the policy network.
\end_layout

\begin_layout Standard
\begin_inset Wrap figure
lines 0
placement o
overhang 0col%
width "50col%"
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/5-Training.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:5-training"

\end_inset

Training curves for the Q-network and the policy network of the minimal
 DDPG agent.
 The return is also plotted with a moving average smoothing.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:5-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can see the Q-network and policy network training curves with batches
 of size 128 over 1000 episodes, as well as return per episode.
 We observe that the trainings of the two networks are correlated.
 Initially, both networks overfit and then rapidly decrease their loss in
 
\begin_inset Formula $\sim100$
\end_inset

 episodes.
 Afterward, the curve plateaus for around 
\begin_inset Formula $100$
\end_inset

 episodes, and finally, both networks learn slowly but steadily until the
 loss reaches 
\begin_inset Formula $\sim0$
\end_inset

 at episode 500.
 We observe that the networks indeed learned well because the return per
 episode increases steadily until episode 500, after which it plateaus.
 This indicates that stopping the training at episode 500 would have been
 a good choice.
\end_layout

\begin_layout Standard
The trained policy achieves an average return and standard deviation over
 100 episodes of 
\begin_inset Formula $-180\pm100$
\end_inset

, which is significantly better than the heuristic policy.
 The comparison can be seen in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:appendix-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
We also want to compare the Q-values given by the Q-network trained on the
 heuristic policy and the agent trained with a policy network.
 The results are shown in the two bottom rows of Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:4-5-Heatmaps"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Firstly, we observe that with the policy network, the high-reward sector
 is much larger than with the heuristic policy.
 This is due to the fact that the policy network is more versatile and can
 react to a wider range of situations.
 In particular, for a null or small velocity, the value of the torque has
 only a small influence on the location of the high-reward sector because
 if the torque is not optimal with respect to the velocity, the policy network
 will compensate for this in the next step, i.e almost immediately.
 This is not the case with the heuristic policy, which is deterministic.
\end_layout

\begin_layout Standard
With velocity=5, we would again expect the two heatmaps to be the mirror
 versions of each other.
 Even though the high-reward sectors are indeed mirrored, with negative
 torque, it is larger and there exists a region with low-reward in the upper-lef
t region, which is nonexistent in the other heatmap.
 This might be due to the fact that a velocity of magnitude 5 in the upper
 quadrants was a rare situation during the training, and thus the Q-network
 has not been able to learn the Q-values well for these situations.
 This is an example of missgeneralization
\begin_inset Foot
status open

\begin_layout Plain Layout
Shah, Rohin, et al.
 
\shape italic
Goal Misgeneralization: Why Correct Specifications Arenât Enough For Correct
 Goals.

\shape default
 2 Nov.
 2022, https://arxiv.org/pdf/2210.01790.pdf.
\end_layout

\end_inset

.
\end_layout

\begin_layout Section*
6 Target networks
\end_layout

\begin_layout Standard
When experimenting with the first implementation of DDPG above, we noticed
 that depending on the choice of the seed, the training was sometimes unstable.
 This motivates the use of target networks, which are copies of the actor
 and critic networks that are "soft updated".
 This means that at each step, they are updated with a weighted average
 between the weights of the target network and the weights of the original
 network.
 In other words, each weight 
\begin_inset Formula $w_{target}$
\end_inset

 of the actor/critic target network are updated using this equation:
\begin_inset Formula 
\[
w_{\mathrm{target}}^{(t+1)}=\tau\cdot w_{\mathrm{original}}^{(t)}+(1-\tau)\cdot w_{\mathrm{target}}^{(t)}
\]

\end_inset

with 
\begin_inset Formula $\tau\in(0,1)$
\end_inset

.
 The target networks are used instead of the true networks in the computation
 of the target Q-value.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang french
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/6-Training.pdf
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:6-training"

\end_inset

Training curves for the Q-network and the policy network, and return per
 episode, for the DDPG agent equipped with target networks, for different
 values of update parameter 
\begin_inset Formula $\tau$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/7-Training.pdf
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:7-training"

\end_inset

Training curves for the Q-network and the policy network, and return per
 episode, for the DDPG agent equipped with target networks (update parameter
 
\begin_inset Formula $\tau=0.5$
\end_inset

), and Ornstein-Uhlenbeck noise, for different values of correlation parameter
 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We implemented this modification, and we see in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:6-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 the training curves of the Q-network and policy network, as well as the
 return per episode, using batch size 128 over 1000 episodes, for 5 values
 of 
\begin_inset Formula $\tau$
\end_inset

.
 
\end_layout

\begin_layout Standard
We see that the stability of the training varies with the value of 
\begin_inset Formula $\tau$
\end_inset

.
 For 
\begin_inset Formula $\tau=0.2$
\end_inset

 and 
\begin_inset Formula $\tau=1$
\end_inset

 (which corresponds to the original DDPG algorithm), the training does not
 converge, with the Q-network loss oscillating around 1000.
 For 
\begin_inset Formula $\tau=0.01$
\end_inset

, the initial increase of the loss, due to overfitting, is slower because,
 by design, a small update parameter slows down the update of the networks.
 Then, it shows a similar behaviour as the 
\begin_inset Formula $\tau=0.2$
\end_inset

 case up to episode 200 until the losses start decreasing to approach zero.
 Hence, despite the observed convergence, it looks likely that with another
 seed, the losses would have continued oscillating.
 For 
\begin_inset Formula $\tau=0.8$
\end_inset

, the training curves show several abrupt peaks during the first 600 episodes.
 The most stable training is obtained for 
\begin_inset Formula $\tau=0.5$
\end_inset

, with the losses decreasing smoothly and the return increasing faster than
 for the other values of 
\begin_inset Formula $\tau$
\end_inset

.
 Therefore, it looks like a sweet spot between larger values that do not
 allow a sufficient effect of the target networks and smaller values that
 update the networks too slowly, making them unable to converge.
\end_layout

\begin_layout Standard
The resulting average return and standard deviation over 100 test episodes
 for each of the five trained policies are: 
\begin_inset Formula $-170\pm90$
\end_inset

 (
\begin_inset Formula $\tau=0.01$
\end_inset

), 
\begin_inset Formula $-1510\pm110$
\end_inset

 (
\begin_inset Formula $\tau=0.2$
\end_inset

), 
\begin_inset Formula $-140\pm80$
\end_inset

 (
\begin_inset Formula $\tau=0.5$
\end_inset

), 
\begin_inset Formula $-150\pm80$
\end_inset

 (
\begin_inset Formula $\tau=0.8$
\end_inset

), and 
\begin_inset Formula $-1480\pm60$
\end_inset

 (
\begin_inset Formula $\tau=1$
\end_inset

).
 In Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:appendix-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we compare the performance of the policy trained with 
\begin_inset Formula $\tau=0.5$
\end_inset

 with the other policies of this work.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
7 Ornstein-Uhlenbeck noise
\end_layout

\begin_layout Standard
\begin_inset Wrap figure
lines 0
placement o
overhang 0col%
width "50col%"
status open

\begin_layout Plain Layout
\begin_inset VSpace -30bp
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/Appendix-Average-return.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:appendix-comparison"

\end_inset

Comparison of average return and standard deviation over 100 episodes for
 the random policy, the heuristic policy with best torque intensity (=1),
 DDPG agent, DDPG agent with target network with best update parameter (
\begin_inset Formula $\tau=0.5$
\end_inset

), and DDPG agent with target network (
\begin_inset Formula $\tau=0.5$
\end_inset

) and Ornstein-Uhlenbeck noise with best correlation parameter (
\begin_inset Formula $\theta=0.5$
\end_inset

).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Adding noise to the action helps exploration of the state space, which is
 necessary to learn a good policy.
 However, adding uncorrelated noise to subsequent actions is not very effective
 in practice, as it leads to trajectories close to the deterministic one.
 To improve the exploration of the algorithm, we replace the Gaussian noise
 with a (simplified) Ornstein-Uhlenbeck noise defined by the iterative equation:
\begin_inset Formula 
\[
\mathrm{noise}^{(n+1)}=(1-\theta)\cdot\mathrm{noise}^{(n)}+\mathcal{N}(0,\sigma^{2})
\]

\end_inset

 with 
\begin_inset Formula $\theta\in(0,1)$
\end_inset

.
\end_layout

\begin_layout Standard
Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:7-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the training curves and return per episode during training of a DDPG
 agent with target networks and Ornstein-Uhlenbeck noise, using batch size
 128 over 1000 episodes.
 The training was done for 5 different values of 
\begin_inset Formula $\text{\ensuremath{\theta}}.$
\end_inset


\end_layout

\begin_layout Standard
We observe that with 
\begin_inset Formula $\theta=0$
\end_inset

, the training looks chaotic.
 This can be explained by a probabilistic phenomenon.
 The noise added at the 
\begin_inset Formula $n$
\end_inset

-th step can be written as 
\begin_inset Formula 
\[
X_{n}\sim\mathcal{N}(0,\sigma^{2})+\sum_{i=1}^{n-1}(1-\theta)^{2i}\mathcal{N}(0,\sigma^{2})\overset{d}{=}\mathcal{N}\left(0,\sigma^{2}\frac{1-(1-\theta)^{2n}}{1-(1-\theta)^{2}}\right)
\]

\end_inset

for 
\begin_inset Formula $\theta>0$
\end_inset

 and 
\begin_inset Formula $\mathcal{N}(0,n\sigma^{2})$
\end_inset

 for 
\begin_inset Formula $\theta=0$
\end_inset

.
 Hence, in the case 
\begin_inset Formula $\theta=0$
\end_inset

, the variance grows linearly with 
\begin_inset Formula $n$
\end_inset

 and, already at the 25th step of an episode, the probability for the noise
 to be greater than 1 in absolute value, causing the action to be likely
 clipped to the boundaries of the action space (
\begin_inset Formula $\pm1$
\end_inset

), is 
\begin_inset Formula $\sim0.5$
\end_inset

, and this probability will continue to grow during the episode.
 The noise thus completely dwarfs the action chosen by the policy.
 As a result, the vast majority of actions taken during training have a
 value 
\begin_inset Formula $\pm1$
\end_inset

, chosen randomly.
 Under this condition, it is no surprise that the agent is unable to learn
 and achieve good performances.
\end_layout

\begin_layout Standard
With a non-null 
\begin_inset Formula $\theta$
\end_inset

, however, while still growing with 
\begin_inset Formula $n$
\end_inset

, the variance is bounded due to the exponential decay of the cumulative
 noise history, and the distribution tends toward 
\begin_inset Formula $X_{n\rightarrow\infty}\sim\mathcal{N}\left(0,\frac{\sigma^{2}}{1-(1-\theta)^{2}}\right)$
\end_inset

.
 In the case 
\begin_inset Formula $\theta=0.25$
\end_inset

, even in the limit 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

, the probability of 
\begin_inset Formula $|X_{\infty}|>1$
\end_inset

 is 
\begin_inset Formula $\sim0.02$
\end_inset

.
\end_layout

\begin_layout Standard
On the other hand, when using large 
\begin_inset Formula $\theta$
\end_inset

 values (
\begin_inset Formula $0.75$
\end_inset

 and 
\begin_inset Formula $1$
\end_inset

), the noise is (almost) uncorrelated, and we recover the situation of the
 previous section.
 However, even though we used the best update parameter found in section
 6 (
\begin_inset Formula $\tau=0.5$
\end_inset

), we observe that the training can still fail to converge.
\end_layout

\begin_layout Standard
The fastest learnings are achieved by small but non-zero values of 
\begin_inset Formula $\theta$
\end_inset

 (0.25 and 0.5).
 This shows the crucial role played by the value of the correlation parameter
 
\begin_inset Formula $\theta$
\end_inset

 in effectively introducing correlation in noise while ensuring that the
 policy's decisions remain significant.
\end_layout

\begin_layout Standard
The resulting average return and standard deviation over 100 test episodes
 for each of the five trained policies are: 
\begin_inset Formula $-1110\pm370$
\end_inset

 (
\begin_inset Formula $\theta=0$
\end_inset

), 
\begin_inset Formula $-170\pm100$
\end_inset

 (
\begin_inset Formula $\theta=0.25$
\end_inset

), 
\begin_inset Formula $-150\pm80$
\end_inset

 (
\begin_inset Formula $\theta=0.5$
\end_inset

), 
\begin_inset Formula $-970\pm470$
\end_inset

 (
\begin_inset Formula $\theta=0.75$
\end_inset

), and 
\begin_inset Formula $-150\pm90$
\end_inset

 (
\begin_inset Formula $\theta=1$
\end_inset

).
 In Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:appendix-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we compare the performance of the policy trained with 
\begin_inset Formula $\theta=0.5$
\end_inset

 with the other policies of this work.
\end_layout

\begin_layout Section*
Conclusion
\end_layout

\begin_layout Standard
In this project, we implemented the DDPG algorithm from scratch to solve
 the classical control problem of stabilising an inverted pendulum.
 First, we trained a Q-network on a heuristic policy, and then we trained
 an actor-critic network using the DDPG algorithm.
 We then studied the importance of using target networks to stabilise the
 learning process and found a sweet spot for the update parameter of the
 target network, 
\begin_inset Formula $\tau=0.5$
\end_inset

.
 We also experimented with using Ornstein-Uhlenbeck noise to explore the
 action space and explained mathematically why a null correlation parameter
 prevents the agent from learning.
 Finally, we compared the performance of all the policies used in this project
 in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:appendix-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We observe that the DDPG algorithm is better than the heuristic policy,
 and although adding target networks and Ornstein-Uhlenbeck noise does not
 lead to better performance, they help stabilise and accelerate the learning
 process, respectively.
\end_layout

\end_body
\end_document
