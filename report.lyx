#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center} 
\backslash
huge
\backslash
textbf{Miniproject - DDPG}
\backslash

\backslash
%
\end_layout

\begin_layout Plain Layout


\backslash
large
\backslash
sffamily Antoine Maier, Aude Maier
\backslash

\backslash
%
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
In this project, we will explore the Deep Deterministic Policy Gradient
 (DDPG) algorithm, which is designed to handle continuous action spaces
 in Reinforcement Learning.
 We will use the 
\begin_inset CommandInset href
LatexCommand href
name "Pendulum-v1"
target "https://www.gymlibrary.dev/environments/classic_control/pendulum/"
literal "false"

\end_inset

 environment implemented in OpenAI Gym to implement the DDPG algorithm from
 scratch to solve the classical control problem of stabilizing an inverted
 pendulum.
 Throughout the development, we will incrementally build the components
 of DDPG and analyze their importance for correct and effective learning.
\end_layout

\begin_layout Section*
3 Heuristic Policy
\end_layout

\begin_layout Standard
In this work, the action space is renormalized to 
\begin_inset Formula $[-1,1]$
\end_inset

 where -1 corresponds to the maximum torque in the clockwise direction (i.e.
 -2) and 1 to the maximum torque in the counterclockwise direction (i.e.
 2).
 This convention is used throughout the rest of this report.
\end_layout

\begin_layout Standard
First we will compare two policies: a random policy that selects actions
 uniformly at random from the action space, and a heuristic policy desined
 to accelerate the pendulum when on the lower half of the domain and deccelerate
 it otherwise.
\end_layout

\begin_layout Standard
The average return over 100 episodes and its standard deviation for the
 random policy is 
\begin_inset Formula $-1230\pm270$
\end_inset

, which sets a baseline for the performance of the policies.
\end_layout

\begin_layout Standard
On Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we see the average return over 100 episodes for the heuristic policy depending
 on the magnitude of the fixed torque.
 The performance increases linearly from 
\begin_inset Formula $-1190\pm330$
\end_inset

 for a null fixed torque to 
\begin_inset Formula $-590\pm200$
\end_inset

 for a fixed torque of 
\begin_inset Formula $0.5$
\end_inset

, and then increases with a smaller slope up to 
\begin_inset Formula $-430\pm110$
\end_inset

 for a fixed torque of 
\begin_inset Formula $1$
\end_inset

.
 The exact performance varies from one run to another, but when the performance
 increases the standard deviation decreases.
 We observe that the heuristic policy performs better than the random policy
 for all nonzero values of the fixed torque.
\end_layout

\begin_layout Standard
The performance of these two policies can be compared to the next policies
 of this homework in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:appendix-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Section*
4 Q-function of the heuristic policy
\end_layout

\begin_layout Standard
We now implement a critic, i.e.
 a Q-network, whose goal is to estimate the Q-function of the heuristic
 policy.
 We use a neural network with two hidden layers of 32 neurons each, with
 ReLU activation functions.
 It is updated using the semi-gradient of the mean squared error between
 the predicted and target Q-values.
 The training is done over 1000 epochs, with a batch size of 64, and a torque
 magnitude of 1 (i.e.
 the value that obtained the highest cumulative reward in the previous section).
\end_layout

\begin_layout Standard
On Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:4-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we see the training curve with batches of size 128 and 1000 epochs.
 We can see that the training is very fast, with the loss decreasing to
 a value of approximately 70 after only 100 episodes.
 The loss then decreases more slowly, reaching a value close to 50 after
 1000 episodes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang french
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/3-Average-return.pdf
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:3"

\end_inset

Average return of the heuristic agent and the random agent over 100 episodes.
 The heuristic agent is trained with 11 different torque intensities between
 0 and 1.
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/4-Training.pdf
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:4-training"

\end_inset

Mean Q-loss per episode during training of the Q-network for the heuristic
 agent.
 The slope of the linear regression line is 0.01.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/4-5-Heatmaps.png
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:4-5-Heatmaps"

\end_inset

Q-value as a function of the position of the pendulum, for a given torque
 and velocity.
 The two first rows plot the Q-values computed by the heuristic Q-network
 before and after training, respectively.
 The third row shows the Q-values computed by the Q-network of the DDPG
 agent.
 For each row, five different torque-velocity pairs are considered.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We then plot and compare the Q-values given by the Q-network before and
 after training, for several values of torque and velocity, by mean of heatmaps.
 The result is shown on the two top rows on Fig.Â 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:4-5-Heatmaps"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where a positive velocity corresponds to the pendulum moving in the trigonomet
ric direction and a positive torque corresponds to a torque applied in the
 trigonometric direction.
\end_layout

\begin_layout Standard
The Q-values before training are almost null constants compared to after
 the training.
 In fact, if plotted on a different color scale, we would see that the Q-values
 before training are a mix of randomness and continuity or order 
\begin_inset Formula $O(10^{-2})$
\end_inset

, which is easily explained by the random initialization of the weights
 and the intrinsic continuity of the network, and it is certainly not due
 to the fact that the policy acheives a high return and that the Q-network
 is able to predict it.
\end_layout

\begin_layout Standard
The Q-values after training are 4 order of magnitude larger (in absolute
 value).
 The heatmap corresponding to velocity=0 and torque=0 shows an accurate
 representation of the reward distribution, i.e.
 the closer you are to 
\begin_inset Formula $\alpha=0^{\circ}$
\end_inset

, the more reward you get and the easier it will be to be close to 
\begin_inset Formula $\alpha=0^{\circ}$
\end_inset

 in the future.
 A notable fact is that this representation appeared without any explicit
 training of the action corresponding to torque=0 but is an interpolation
 of the knowledge acquired for torque=
\begin_inset Formula $\pm1$
\end_inset

.
\end_layout

\begin_layout Standard
The heatmaps of velocity=1, torque=-1 is similar to the previous one, but
 with a slight rotation in the clockwise direction.
 This is due to the fact that with this (small) velocity, the heuristic
 policy will decelerate the pendulum to try to balance it in the inverted
 position, thus leading to a high reward for a small negative angle.
 We note that the lower left and right cadrans are situations that the Q-network
 has never experienced, since the heuristic policy is perfectly deterministic.
 With velocity=1 but torque=1, these are the two upper cadrans that are
 unexperienced situations for the Q-network.
 However, the heatmap is very similar to the previous one, hence it looks
 the Q-network has infered the Q-values for the unexperienced situations
 from the experienced ones.
 Unfortunately, we can doubt that in such a situation of positive velocity
 and accelerated pendulum the Q-value would be as high as calimed by the
 Q-network.
\end_layout

\begin_layout Standard
Then, the heatmap with velocity=5 and torque=-1 resembles the one with velocity=
1 and torque=-1, but with the high-reward circular sector rotated in the
 clockwise direction.
 This is due to the fact that with a greater velocity, the heuristic policy
 will be able balance the pendulum if it decelerates it earlier.
 This time, we chose to change sign of both the velocity and the torque.
 We expect the heatmap to be the mirrored image of the previous one, wich
 is the case up to a certain extent.
 We can image that the differrences between the two heatmaps is due to the
 fact that the Q-network has not been perfectly trained.
\end_layout

\begin_layout Section*
5 Minimal implementation of DDPG
\end_layout

\begin_layout Standard
In this section, we replace the heuristic agent by an agent equipped with
 a policy network that compute the next action to take based on the current
 state.
 We use a neural network with two hidden layers of 32 neurons each, with
 ReLU activation functions, and a tanh activation on the output layer, ensuring
 the returned action is between -1 and 1.
 The policy network is updated in order to maximize the mean Q-value.
 The simultaneous training of the policy network and the Q network is done
 over 1000 epochs, with a batch size of 128.
 To enable a better exploration of the state space when generating new transitio
ns, a small Gaussian noise is added to the action returned by the policy
 network.
\end_layout

\begin_layout Standard
\begin_inset Wrap figure
lines 0
placement o
overhang 0col%
width "50col%"
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/5-Training.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:5-training"

\end_inset

Training curves for the Q-network and the policy network of the minimal
 DDPG agent.
 The return is also plotted with a moving average smoothing.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
On Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:5-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we see the Q-network and policy network training curves with batches of
 size 128 over 1000 episodes, as well as return per episode.
 We see that the training of the two networks are correlated, they both
 first overfit then rapidly decerse their loss in ~100 episodes, then the
 curve plateau for ~100 episodes, and finally they both learn slowly but
 steadily until the loss reaches ~0 at episode 500.
 We see that indeed the networks learned well because the return per episode
 increases steadily until episode 500, then it plateaus.
 This means that stopping the training at episode 500 would have been a
 good choice.
\end_layout

\begin_layout Standard
The trained policy achieves an average return over 100 episodes and standard
 deviation of 
\begin_inset Formula $-180\pm100$
\end_inset

, which is significantly better than the heuristic policy.
 The comparison can be seeon on Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:appendix-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
We also want to compare the Q-values given by the Q-network trained on the
 heuristic policy and trained on the agent with a policy network.
 The result is shown on the two bottom rows of Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:4-5-Heatmaps"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 First, we see that with the policy network the high-rewar sector is much
 larger than with the heuristic policy.
 This is due to the fact that the policy network is more versatile and thus
 can react to more various situations.
 In particular, for a null or small velocity, the value of the torque has
 only a small influence on the location of the high-reward sector because
 if the torque is not optimal with respect to the velocity, the policy network
 will compensate this the next step, i.e almost immediately.
 This is not the case with the heuristic policy, which is deterministic.
\end_layout

\begin_layout Standard
With velocity=5, we would again expect the two heatmaps to be the mirror
 version of each other.
 Even though the high-reward sectors are indeed mirrored, with negative
 torque is is larger and there exists a region with low-reward in the upper-left
 region, which is nonexistent in the other heatmap.
 This might be due to the fact that a velocity of magnitude 5 in the upper
 cadrans was a rare situations during the training and thus the Q-network
 has not been able to learn well the Q-values for these situations, which
 is an example of missgeneralization[SOURCE].
\end_layout

\begin_layout Section*
6 Target networks
\end_layout

\begin_layout Standard
When experimenting with the first implementation of DDPG above, we noticed
 that depending on the choice of the seed, the training was sometimes unstable.
 This motivates the use of target networks, which are copies of the actor
 and critic networks that are "soft updated".
 This means that each step they are updated with a weighted average between
 the weights of the target network and the weights of the original network,
 i.e.
 each weight 
\begin_inset Formula $w_{target}$
\end_inset

 of the actor/critic target network are updated using this equation:
\begin_inset Formula 
\[
w_{\mathrm{target}}^{(t+1)}=\tau\cdot w_{\mathrm{original}}^{(t)}+(1-\tau)\cdot w_{\mathrm{target}}^{(t)}
\]

\end_inset

with 
\begin_inset Formula $\tau\in(0,1)$
\end_inset

.
 The target networks are used instead of the true networks in the computation
 of the target Q-value.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang french
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/6-Training.pdf
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:6-training"

\end_inset

Training curves for the Q-network and the policy network of the DDPG agent
 equipped with target networks, for different values of update parameter
 
\begin_inset Formula $\tau$
\end_inset

.
 The return per episode is also plotted for each value of 
\begin_inset Formula $\tau.$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/7-Training.pdf
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:7-training"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We implemented this modification and we see on Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:6-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 the training curves of the Q-network and policy network, as well as the
 return per episode, using batch size 128 over 1000 episodes, for 5 values
 of 
\begin_inset Formula $\tau$
\end_inset

.
 
\end_layout

\begin_layout Standard
We see that the stability of the training varies with the value of 
\begin_inset Formula $\tau$
\end_inset

.
 For 
\begin_inset Formula $\tau=0.1$
\end_inset

 and 
\begin_inset Formula $\tau=1$
\end_inset

 (which corresponds to the original DDPG algorithm), the training is unstable,
 with the Q-network loss and policy network loss oscillating around 1000.
 For 
\begin_inset Formula $\tau=0.01$
\end_inset

, the curves are similar to 
\begin_inset Formula $\tau=0.1$
\end_inset

 during the first 200 episodes but then the losses decrease to approach
 zero.
 Hence, despite the observed convergence, it is likely that another seed
 could have make the training unstable.
 For 
\begin_inset Formula $\tau=0.8$
\end_inset

, the training curves show several abrupt peaks during the first 600 episodes.
 The most stable training is obtained for 
\begin_inset Formula $\tau=0.5$
\end_inset

, with the losses decreasing smoothly and the return increasing faster than
 for the other values of 
\begin_inset Formula $\tau$
\end_inset

.
 
\begin_inset Formula $\tau=0.5$
\end_inset

 therefore looks like a sweet spot between larger values that do not allow
 a sufficient effect of the target networks and smaller values that update
 the networks too slowly, making them unable to converge.
\end_layout

\begin_layout Standard
...
 add the average cumulative reward over 100 episodes...
\end_layout

\begin_layout Section*
7 Ornstein-Uhlenbeck noise
\end_layout

\begin_layout Standard
Adding noise to the action helps exploration of the state space, which is
 necessary to learn a good policy.
 However, adding uncorrelated noise to subsequent actions is not very effective
 in practice, as it leads to trajectories close to the deterministic one.
 To improve the exploration of the algorithm, we replace the Gaussian noise
 with a (simplified) Ornstein-Uhlenbeck noise defined by the iterative equation:
\begin_inset Formula 
\[
\mathrm{noise}^{(t+1)}=(1-\theta)\cdot\mathrm{noise}^{(t)}+\mathcal{N}(0,\sigma^{2})
\]

\end_inset

 with 
\begin_inset Formula $\theta\in(0,1)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/Appendix-Average-return.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:appendix-comparison"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
