#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Miniproject - DDPG
\end_layout

\begin_layout Author
Antoine Maier - Aude Maier
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
In this project, we will explore the Deep Deterministic Policy Gradient
 (DDPG) algorithm, which is designed to handle continuous action spaces
 in Reinforcement Learning.
 We will use the 
\begin_inset CommandInset href
LatexCommand href
name "Pendulum-v1"
target "https://www.gymlibrary.dev/environments/classic_control/pendulum/"
literal "false"

\end_inset

 environment implemented in OpenAI Gym to implement the DDPG algorithm from
 scratch to solve the classical control problem of stabilizing an inverted
 pendulum.
 Throughout the development, we will incrementally build the components
 of DDPG and analyze their importance for correct and effective learning.
 
\end_layout

\begin_layout Standard
This Jupyter notebook contains our implementation and report for this project.
 Do not forget to remove the report parts before submitting the notebook.
\end_layout

\begin_layout Standard
The instructions are available in `Miniproject_DDPG.pdf` file.
\end_layout

\begin_layout Section*
3 Heuristic Policy
\end_layout

\begin_layout Standard
The environment consists of a pendulum attached to a fixed pivot point.
 Its position is described by the angle 
\begin_inset Formula $\alpha$
\end_inset

 between the pendulum and the upward vertical direction, and its (angular)
 velocity is 
\begin_inset Formula $\dot{\alpha}$
\end_inset

.
 The pendulum is actuated by applying a torque 
\begin_inset Formula $\tau$
\end_inset

 on its pivot point (the action space is thus continuous).
 The pendulum is subject to gravity, which is the only external force acting
 on it.
\end_layout

\begin_layout Standard
The pendulum starts in a random position, with a random velocity and the
 aim is to stabilize it in the inverted position, using little torque.
 More precisely, we want to maximise cummulative reward 
\begin_inset Formula $-\left(\alpha^{2}+0.1\dot{\alpha}^{2}+0.001\tau^{2}\right)$
\end_inset

 over a time horizon of 200 steps.
 The pendulum starts in a random position, with a random velocity.
\end_layout

\begin_layout Standard
First we will compare two policies: a random policy and a heuristic policy.
 The random policy is a policy that selects actions uniformly at random
 from the action space, i.e.
 it applies a tork of random magnitude (within the allowed range)in a random
 direction.
 The heuristic policy is defined as follows:
\end_layout

\begin_layout Itemize
When the pendulum is in the lower half of the domain, applies a fixed torque
 in the same direction as the pendulum's angular velocity;
\end_layout

\begin_layout Itemize
When the pendulum is in the upper half of the domain, applies a fixed torque
 in the opposite direction as the pendulum's angular velocity.
\end_layout

\begin_layout Standard
The average return over 100 episodes and its standard deviation for the
 random policy is 
\begin_inset Formula $-1230\pm270$
\end_inset

, which sets a baseline for the performance of the policies.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang french
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/3-Average-return.pdf
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:3"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/4-Training.pdf
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:4-training"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
On Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we see the average return over 100 episodes for the heuristic policy depending
 on the magnitude of the fixed torque.
 The performance increases linearly from 
\begin_inset Formula $-1190\pm330$
\end_inset

 for a null fixed torque to 
\begin_inset Formula $-590\pm200$
\end_inset

 for a fixed torque of 
\begin_inset Formula $0.5$
\end_inset

, and then increases with a smaller slope up to 
\begin_inset Formula $-430\pm110$
\end_inset

 for a fixed torque of 
\begin_inset Formula $1$
\end_inset

.
 The exact performance varies from one run to another, but when the performance
 increases the standard deviation decreases.
 We observe that the heuristic policy performs better than the random policy
 for all nonzero values of the fixed torque.
\end_layout

\begin_layout Section*
4 Q-function of the heuristic policy
\end_layout

\begin_layout Standard
We now implement a critic, i.e.
 a Q-network, whose goal is to estimate the Q-function of the heuristic
 policy.
 We use a neural network with two hidden layers of 32 neurons each, with
 ReLU activation functions.
 It is updated using the semi-gradient of the mean squared error between
 the predicted and target Q-values.
 The training is done over 1000 epochs, with a batch size of 64, and a torque
 magnitude of 1 (i.e.
 the value that obtained the highest cumulative reward in the previous section).
\end_layout

\begin_layout Standard
On Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:4-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we see the training curve with batches of size 128 and 1000 epochs.
 We can see that the training is very fast, with the loss decreasing to
 a value of approximately 70 after only 100 episodes.
 The loss then decreases more slowly, reaching a value close to 50 after
 1000 episodes.
\end_layout

\begin_layout Standard
We then plot and compare the Q-values given by the Q-network before and
 after training, for several values of torque and velocity, by mean of heatmaps.
 The chosen values for the torque and velocity are 
\begin_inset Formula $(0,$
\end_inset


\end_layout

\begin_layout Standard
As can be seen on Fig.
\begin_inset space ~
\end_inset

, the Q-values before training are a mix of randomness and continuity, which
 is easily explained by the random initialization of the weights and the
 intrinsic continuity of the network.
 The Q-values after training are (in absolute value) larger by 4 order of
 magnitudes.
 The heatmap corresponding to 0 torque and 0 velocity shows an accurate
 representation from the network of the reward distribution, i.e.
 the closer you are to 
\begin_inset Formula $0^{Â°}$
\end_inset

, the more reward you get and the easier it will be to be close to 
\begin_inset Formula $0^{Â°}$
\end_inset

 in the future.
 A notable fact is that this representation appeared without any explicit
 training of the action corresponding to torque
\begin_inset Formula $=0$
\end_inset

 but is an interpolation of the knowledge acquired for torque
\begin_inset Formula $=\pm1$
\end_inset

.
\end_layout

\begin_layout Standard
The heatmaps of torque
\begin_inset Formula $=\pm1$
\end_inset

, velocity
\begin_inset Formula $=-1$
\end_inset

 are very similar and can be summarized as a slight rotation of the 
\begin_inset Formula $(0,0)$
\end_inset

 heatmap.
 Again, one can note that each torque has only been trained on half of the
 angle values, hence they look as if they have inspired each other for the
 other half of the values.
\end_layout

\begin_layout Standard
Finally, the heatmaps with torque
\begin_inset Formula $=\pm1$
\end_inset

, velocity
\begin_inset Formula $=-5$
\end_inset


\end_layout

\begin_layout Section*
5 Minimal implementation of DDPG
\end_layout

\begin_layout Standard
In this section, we replace the heuristic agent by an agent equipped with
 a policy network that compute the next action to take based on the current
 state.
 We use a neural network with two hidden layers of 32 neurons each, with
 ReLU activation functions, and a tanh activation on the output layer, ensuring
 the returned action is between -1 and 1.
 The policy network is updated in order to maximize the mean Q-value.
 The simultaneous training of the policy network and the Q network is done
 over 1000 epochs, with a batch size of 128.
 To enable a better exploration of the state space when generating new transitio
ns, a small Gaussian noise is added to the action returned by the policy
 network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/5-Training.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:5-training"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:5-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 reports the losses of the Q network and Policy network as well as the cumulativ
e reward for each episode throughout the training.
\end_layout

\begin_layout Standard
Once the training is done, we execute 100 test episodes uding the trained
 Policy network.
 The average return obtained is...
\end_layout

\begin_layout Section*
6 Target networks
\end_layout

\begin_layout Standard
When experimenting with the first implementation of DDPG above, we noticed
 that depending on the choice of the seed, the training was sometimes unstable.
 This motivates the use of target networks, which are copies of the actor
 and critic networks that are "soft updated".
 This means that each step they are updated with a weighted average between
 the weights of the target network and the weights of the original network,
 i.e.
 each weight 
\begin_inset Formula $w_{target}$
\end_inset

 of the actor/critic target network are updated using this equation:
\begin_inset Formula 
\[
w_{\mathrm{target}}^{(t+1)}=\tau\cdot w_{\mathrm{original}}^{(t)}+(1-\tau)\cdot w_{\mathrm{target}}^{(t)}
\]

\end_inset

with 
\begin_inset Formula $\tau\in(0,1)$
\end_inset

.
 The target networks are used instead of the true networks in the computation
 of the target Q-value.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang french
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/6-Training.pdf
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\lang french
\begin_inset Graphics
	filename //wsl$/Ubuntu/home/aude/workspace_vscode/ANN/DDPG-Inverted-Pendulum/Figures/7-Training.pdf
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout

\lang french
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section*
7 Ornstein-Uhlenbeck noise
\end_layout

\begin_layout Standard
Adding noise to the action helps exploration of the state space, which is
 necessary to learn a good policy.
 However, adding uncorrelated noise to subsequent actions is not very effective
 in practice, as it leads to trajectories close to the deterministic one.
 To improve the exploration of the algorithm, we replace the Gaussian noise
 with a (simplified) Ornstein-Uhlenbeck noise defined by the iterative equation:
\begin_inset Formula 
\[
\mathrm{noise}^{(t+1)}=(1-\theta)\cdot\mathrm{noise}^{(t)}+\mathcal{N}(0,\sigma^{2})
\]

\end_inset

 with 
\begin_inset Formula $\theta\in(0,1)$
\end_inset

.
\end_layout

\end_body
\end_document
